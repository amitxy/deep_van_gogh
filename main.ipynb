{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "#from platform import architecture\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.f2py.cfuncs import includes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sympy.codegen import Print\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from optuna import trial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import optuna\n",
    "import wandb\n",
    "# Project utilities\n",
    "import utils\n",
    "from train import train_model_with_hyperparams\n",
    "\n",
    "VGG19 = 'VGG19'\n",
    "ALEXNET = 'AlexNet'\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms = True"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Check if you're working locally or not\n",
    "if not (os.path.exists(utils.CSV_PATH) and os.path.exists(utils.OPTIMIZED_DIR)):\n",
    "    print(f\"[!] You are NOT on the project's directory [!]\\n\"\n",
    "          f\"Please run the following command (in either CMD or anaconda prompt): \\n\"\n",
    "          f\"jupyter notebook --notebook-dir PROJECT_DIR\\n\"\n",
    "          r\"Where PROJECT_DIR is the project's directory in your computer e.g: C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading our data\n",
    "We will load the optimized datasets from our custom dataset object\n"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "class NumPyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.images = data[\"images\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, 'dataset.npz'))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can find the optimized dataset files <a href=\"https://drive.google.com/drive/folders/1TBlNcRsRHJ7_rxh_h7_yn_-Ak66Uj_mp?usp=sharing\">HERE</a><br/>\n",
    "Loading the train and test datasets:"
   ]
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "classes = pd.read_csv(utils.CSV_PATH)\n",
    "# train_indices, val_indices = train_test_split(classes[classes['subset'] == 'train'].index.tolist(), test_size=0.2, random_state=SEED)\n",
    "train_indices = classes[classes['subset'] != 'test'].index.tolist()\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, classes[classes['subset'] == 'test'].index.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "def get_opt_dataset(dataset_name, indices=None):\n",
    "    ds = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, f'{dataset_name}.npz'))\n",
    "    if indices:\n",
    "        ds = Subset(ds, indices)\n",
    "    return ds\n",
    "\n",
    "flip_dataset = get_opt_dataset('flip', train_indices)\n",
    "dropout_dataset = get_opt_dataset('dropout',train_indices)\n",
    "affine_dataset = get_opt_dataset('affine', train_indices)\n",
    "blur_dataset = get_opt_dataset('blur')\n",
    "\n",
    "augmented_train_dataset = ConcatDataset([train_dataset, flip_dataset, dropout_dataset, affine_dataset])\n",
    "# augmented_train_dataset = train_dataset\n",
    "# train_loader = DataLoader(augmented_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=8)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "source": [
    "# Fine tuning VGG19"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T18:30:28.000734Z",
     "start_time": "2025-03-24T18:30:27.986347Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FinedTunedModel(nn.Module):\n",
    "    def __init__(self, base_model, architecture:str):\n",
    "        super(FinedTunedModel, self).__init__()\n",
    "        self._architecture = architecture  # Save the base model architecture\n",
    "        base_children_list = list(base_model.children())\n",
    "        self.features_extractor = nn.Sequential(*base_children_list[:-1]).to(device)\n",
    "        for param in self.features_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to fit to our problem (2 classes)\n",
    "        self.classifier = nn.Sequential(*base_children_list[-1])\n",
    "        self.classifier[-1] = nn.Linear(4096, 2).to(device)  # Replaces the final layer of the base model's classifier with a new fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_model_output = self.features_extractor(x)\n",
    "        return self.classifier(torch.flatten(base_model_output, start_dim=1))\n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self._architecture\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T18:30:31.984467Z",
     "start_time": "2025-03-24T18:30:28.098369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device) # Load pre-trained VGG19 model\n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)\n",
    "vgg_model = FinedTunedModel(vgg19, VGG19).to(device)\n",
    "vgg_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinedTunedModel(\n",
       "  (features_extractor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:01:46.114222Z",
     "start_time": "2025-03-24T19:01:46.102959Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# len(train_indices)*4\n",
    "# len(augmented_train_dataset)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11696"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:02:02.475699Z",
     "start_time": "2025-03-24T19:02:02.462188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation(dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial):\n",
    "    # Initialize KFold\n",
    "    k_folds = 4\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Track performance for each model\n",
    "    results = defaultdict(list)\n",
    "    base_model = vgg19 if architecture == VGG19 else alexnet\n",
    "\n",
    "    best_values = []\n",
    "    # indices = dataset.indices\n",
    "    # labels = dataset.dataset.labels[indices]\n",
    "    indices = np.arange(len(dataset))\n",
    "    labels = np.tile(train_dataset.dataset.labels[train_dataset.indices],len(dataset.datasets))\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(indices, labels)):\n",
    "        #########\n",
    "        wandb.init(project='deep_van_gogh-aug',\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": 128,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{architecture}_trial_{trial.number + 1}_fold_{fold}\")\n",
    "        ################\n",
    "\n",
    "\n",
    "        train_subset = Subset(dataset, train_ids)\n",
    "        val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=128, pin_memory=True, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=128, pin_memory=True, shuffle=False)\n",
    "        model = FinedTunedModel(base_model.to(device), architecture).to(device)\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Train the model\n",
    "        best_val = train_model_with_hyperparams(model,\n",
    "                                                train_loader,\n",
    "                                                val_loader,\n",
    "                                                optimizer,\n",
    "                                                criterion,\n",
    "                                                epochs=epochs,\n",
    "                                                patience=patience,\n",
    "                                                device=device,\n",
    "                                                trial=trial,\n",
    "                                                architecture=architecture, fold=fold)\n",
    "        wandb.log({'Fold': fold, 'AUC':best_val})\n",
    "        best_values.append(best_val)\n",
    "        # Finish the Weights & Biases run\n",
    "        wandb.finish()\n",
    "\n",
    "    return np.mean(best_values)"
   ],
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:02:02.987145Z",
     "start_time": "2025-03-24T19:02:02.972835Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:02:30.874338Z",
     "start_time": "2025-03-24T19:02:30.858340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optuna objective function\n",
    "def objective(trial, model, config: dict) -> float:\n",
    "    \"\"\"\n",
    "    Generic Optuna objective function.\n",
    "    :param trial: Optuna trial object.\n",
    "    :param model: The neural network model to train\n",
    "    :param config: A dictionary with configurable values such as learning rate ranges, batch size ranges, etc.\n",
    "    :return:  best_val_loss: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions based on config\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\",\n",
    "                                        config.get(\"lr_min\", 1e-5),\n",
    "                                        config.get(\"lr_max\", 1e-3),\n",
    "                                        log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\",\n",
    "                                       config.get(\"wd_min\", 1e-6),\n",
    "                                       config.get(\"wd_max\", 1e-4),\n",
    "                                       log=True)\n",
    "    # batch_size = trial.suggest_int(\"batch_size\",\n",
    "    #                                config.get(\"batch_size_min\", 32),\n",
    "    #                                config.get(\"batch_size_max\", 128),\n",
    "    #                                step=config.get(\"batch_size_step\", 16))\n",
    "    patience = config.get(\"patience\", 8)\n",
    "    epochs = config.get(\"epochs\", 20)\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Load the train DataLoader with the chosen batch_size\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Load the val DataLoader with the chosen batch_size\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = config.get(\"criterion\", nn.CrossEntropyLoss()) # Classification.\n",
    "\n",
    "    # optimizer_class = config.get(\"optimizer_class\", optim.Adam)\n",
    "    # optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    architecture = getattr(model, \"architecture\", model.__class__.__name__)\n",
    "    # wandb.init(project=\"deep_van_gogh\",\n",
    "    #            config={\n",
    "    #     \"learning_rate\": learning_rate,\n",
    "    #     \"weight_decay\": weight_decay,\n",
    "    #     \"patience\": patience,\n",
    "    #     \"batch_size\": batch_size,\n",
    "    #     \"epochs\": epochs,\n",
    "    #     \"architecture\": architecture,\n",
    "    #     \"dataset\": \"Post_Impressionism\"\n",
    "    # },\n",
    "    # name=f\"{architecture}_trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best val_auc\n",
    "    # mean_val = cross_validation(train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\n",
    "    mean_val = cross_validation(augmented_train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\n",
    "    #print(mean_val)\n",
    "\n",
    "    # mean_val = train_model_with_hyperparams(model, train_loader, learning_rate, weight_decay, criterion,\n",
    "    #                                              epochs=epochs, patience=patience, device=device, trial=trial,\n",
    "    #                                              architecture=architecture)\n",
    "\n",
    "\n",
    "\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return mean_val\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-24T19:03:18.196645Z",
     "start_time": "2025-03-24T19:02:32.261262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optuna for our vgg model with the default config\n",
    "study = optuna.create_study(study_name=VGG19, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, vgg_model, config={}), n_trials=15)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-24 21:02:32,263] A new study created in memory with name: VGG19\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: amitr5 (amitr5-tel-aviv-university) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\\wandb\\run-20250324_210234-00343p6q</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug/runs/00343p6q' target=\"_blank\">VGG19_trial_1_fold_0</a></strong> to <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug/runs/00343p6q' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-aug/runs/00343p6q</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-03-24 21:03:17,576] Trial 0 failed with parameters: {'learning_rate': 0.0003578498492933724, 'weight_decay': 2.5628401075200825e-05} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\amitr5\\AppData\\Local\\Temp\\ipykernel_18336\\2319463337.py\", line 3, in <lambda>\n",
      "    study.optimize(lambda trial: objective(trial, vgg_model, config={}), n_trials=15)\n",
      "  File \"C:\\Users\\amitr5\\AppData\\Local\\Temp\\ipykernel_18336\\1015064751.py\", line 51, in objective\n",
      "    mean_val = cross_validation(augmented_train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\n",
      "  File \"C:\\Users\\amitr5\\AppData\\Local\\Temp\\ipykernel_18336\\2664636608.py\", line 39, in cross_validation\n",
      "    best_val = train_model_with_hyperparams(model,\n",
      "  File \"C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\\train.py\", line 49, in train_model_with_hyperparams\n",
      "    outputs = model(inputs)  # Forward pass\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\amitr5\\AppData\\Local\\Temp\\ipykernel_18336\\4267206226.py\", line 15, in forward\n",
      "    base_model_output = self.features_extractor(x)\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"C:\\Users\\amitr5\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "KeyboardInterrupt\n",
      "[W 2025-03-24 21:03:17,580] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Optuna for our vgg model with the default config\u001B[39;00m\n\u001B[0;32m      2\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(study_name\u001B[38;5;241m=\u001B[39mVGG19, direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m \u001B[43mstudy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moptimize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvgg_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m15\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\study.py:475\u001B[0m, in \u001B[0;36mStudy.optimize\u001B[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m    373\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21moptimize\u001B[39m(\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    375\u001B[0m     func: ObjectiveFuncType,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    382\u001B[0m     show_progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    383\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    384\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Optimize an objective function.\u001B[39;00m\n\u001B[0;32m    385\u001B[0m \n\u001B[0;32m    386\u001B[0m \u001B[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    473\u001B[0m \u001B[38;5;124;03m            If nested invocation of this method occurs.\u001B[39;00m\n\u001B[0;32m    474\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 475\u001B[0m     \u001B[43m_optimize\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    476\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstudy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    478\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    479\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    480\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_jobs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mn_jobs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    481\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcatch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mIterable\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    482\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    483\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    484\u001B[0m \u001B[43m        \u001B[49m\u001B[43mshow_progress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mshow_progress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    485\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\_optimize.py:63\u001B[0m, in \u001B[0;36m_optimize\u001B[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001B[0m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m     62\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m---> 63\u001B[0m         \u001B[43m_optimize_sequential\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     64\u001B[0m \u001B[43m            \u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     65\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     66\u001B[0m \u001B[43m            \u001B[49m\u001B[43mn_trials\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     67\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     70\u001B[0m \u001B[43m            \u001B[49m\u001B[43mgc_after_trial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     71\u001B[0m \u001B[43m            \u001B[49m\u001B[43mreseed_sampler_rng\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     72\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtime_start\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m            \u001B[49m\u001B[43mprogress_bar\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprogress_bar\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     75\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     76\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m n_jobs \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\_optimize.py:160\u001B[0m, in \u001B[0;36m_optimize_sequential\u001B[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001B[0m\n\u001B[0;32m    157\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 160\u001B[0m     frozen_trial \u001B[38;5;241m=\u001B[39m \u001B[43m_run_trial\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstudy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfunc\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    162\u001B[0m     \u001B[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001B[39;00m\n\u001B[0;32m    163\u001B[0m     \u001B[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001B[39;00m\n\u001B[0;32m    164\u001B[0m     \u001B[38;5;66;03m# Please refer to the following PR for further details:\u001B[39;00m\n\u001B[0;32m    165\u001B[0m     \u001B[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001B[39;00m\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gc_after_trial:\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\_optimize.py:248\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    241\u001B[0m         \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mShould not reach.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    243\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    244\u001B[0m     frozen_trial\u001B[38;5;241m.\u001B[39mstate \u001B[38;5;241m==\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mFAIL\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m func_err \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    246\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(func_err, catch)\n\u001B[0;32m    247\u001B[0m ):\n\u001B[1;32m--> 248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m func_err\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m frozen_trial\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\optuna\\study\\_optimize.py:197\u001B[0m, in \u001B[0;36m_run_trial\u001B[1;34m(study, func, catch)\u001B[0m\n\u001B[0;32m    195\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m get_heartbeat_thread(trial\u001B[38;5;241m.\u001B[39m_trial_id, study\u001B[38;5;241m.\u001B[39m_storage):\n\u001B[0;32m    196\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 197\u001B[0m         value_or_values \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    198\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m exceptions\u001B[38;5;241m.\u001B[39mTrialPruned \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    199\u001B[0m         \u001B[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001B[39;00m\n\u001B[0;32m    200\u001B[0m         state \u001B[38;5;241m=\u001B[39m TrialState\u001B[38;5;241m.\u001B[39mPRUNED\n",
      "Cell \u001B[1;32mIn[58], line 3\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(trial)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Optuna for our vgg model with the default config\u001B[39;00m\n\u001B[0;32m      2\u001B[0m study \u001B[38;5;241m=\u001B[39m optuna\u001B[38;5;241m.\u001B[39mcreate_study(study_name\u001B[38;5;241m=\u001B[39mVGG19, direction\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmaximize\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m study\u001B[38;5;241m.\u001B[39moptimize(\u001B[38;5;28;01mlambda\u001B[39;00m trial: \u001B[43mobjective\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvgg_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m, n_trials\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m15\u001B[39m)\n",
      "Cell \u001B[1;32mIn[57], line 51\u001B[0m, in \u001B[0;36mobjective\u001B[1;34m(trial, model, config)\u001B[0m\n\u001B[0;32m     36\u001B[0m architecture \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marchitecture\u001B[39m\u001B[38;5;124m\"\u001B[39m, model\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# wandb.init(project=\"deep_van_gogh\",\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m#            config={\u001B[39;00m\n\u001B[0;32m     39\u001B[0m \u001B[38;5;66;03m#     \"learning_rate\": learning_rate,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# Train the model and get the best val_auc\u001B[39;00m\n\u001B[0;32m     50\u001B[0m \u001B[38;5;66;03m# mean_val = cross_validation(train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\u001B[39;00m\n\u001B[1;32m---> 51\u001B[0m mean_val \u001B[38;5;241m=\u001B[39m \u001B[43mcross_validation\u001B[49m\u001B[43m(\u001B[49m\u001B[43maugmented_train_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight_decay\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marchitecture\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrial\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     52\u001B[0m \u001B[38;5;66;03m#print(mean_val)\u001B[39;00m\n\u001B[0;32m     53\u001B[0m \n\u001B[0;32m     54\u001B[0m \u001B[38;5;66;03m# mean_val = train_model_with_hyperparams(model, train_loader, learning_rate, weight_decay, criterion,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     59\u001B[0m \n\u001B[0;32m     60\u001B[0m \u001B[38;5;66;03m# Return best validation loss as the objective to minimize\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mean_val\n",
      "Cell \u001B[1;32mIn[56], line 39\u001B[0m, in \u001B[0;36mcross_validation\u001B[1;34m(dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\u001B[0m\n\u001B[0;32m     37\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39mlearning_rate, weight_decay\u001B[38;5;241m=\u001B[39mweight_decay)\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# Train the model\u001B[39;00m\n\u001B[1;32m---> 39\u001B[0m best_val \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_model_with_hyperparams\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     40\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     41\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     42\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     43\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     44\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     45\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mpatience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     46\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     47\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43mtrial\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrial\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     48\u001B[0m \u001B[43m                                        \u001B[49m\u001B[43marchitecture\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43marchitecture\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfold\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfold\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     49\u001B[0m wandb\u001B[38;5;241m.\u001B[39mlog({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mFold\u001B[39m\u001B[38;5;124m'\u001B[39m: fold, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mAUC\u001B[39m\u001B[38;5;124m'\u001B[39m:best_val})\n\u001B[0;32m     50\u001B[0m best_values\u001B[38;5;241m.\u001B[39mappend(best_val)\n",
      "File \u001B[1;32m~\\PycharmProjects\\deep_van_gogh\\train.py:49\u001B[0m, in \u001B[0;36mtrain_model_with_hyperparams\u001B[1;34m(model, train_loader, val_loader, optimizer, criterion, epochs, patience, device, trial, architecture, fold)\u001B[0m\n\u001B[0;32m     46\u001B[0m inputs, labels \u001B[38;5;241m=\u001B[39m inputs\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m     48\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()  \u001B[38;5;66;03m# Reset gradients\u001B[39;00m\n\u001B[1;32m---> 49\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m     51\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(outputs, labels)  \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[0;32m     52\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()  \u001B[38;5;66;03m# Backward pass\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "Cell \u001B[1;32mIn[7], line 15\u001B[0m, in \u001B[0;36mFinedTunedModel.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m     14\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m---> 15\u001B[0m     base_model_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeatures_extractor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(torch\u001B[38;5;241m.\u001B[39mflatten(base_model_output, start_dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m))\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1737\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1738\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1739\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1745\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1746\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1747\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1748\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1749\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1750\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1752\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1753\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    249\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 250\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m module(\u001B[38;5;28minput\u001B[39m)\n\u001B[0;32m    251\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-Validation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T15:53:51.002715Z",
     "start_time": "2025-03-23T15:53:50.982612Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 14,
   "source": [
    "def cross_validation_temp(dataset:Dataset, **models_dict):\n",
    "    # Initialize KFold\n",
    "    k_folds = 5\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Track performance for each model\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "            #print(f\"\\tFold {fold + 1}\")\n",
    "            # Subset the dataset for this fold\n",
    "            train_subset = Subset(dataset, train_ids)\n",
    "            val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "            for model_name, model_dict in models_dict.items():\n",
    "                #print(f\"Training :{model_name}\")\n",
    "                # Load the entire model\n",
    "                base_model = vgg19 if model_dict[\"architecture\"] == VGG19 else alexnet\n",
    "                model = FinedTunedModel(base_model.to(device), model_dict[\"architecture\"]).to(device)\n",
    "\n",
    "                # Define loss function and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), **model_dict['param_groups'])\n",
    "\n",
    "\n",
    "\n",
    "                # Train the model (implement your training loop here)\n",
    "                best_val = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion,\n",
    "                                                             epochs=3, patience=3, device=device, trial=None,\n",
    "                                                             architecture=None)\n",
    "                 # Append the results for this fold\n",
    "                results[model_name].append(best_val)\n",
    "\n",
    "\n",
    "    # Print final results\n",
    "    mean_perf_dict = {}\n",
    "    for model_name, model_results in results.items():\n",
    "         # After all folds, calculate the average fold performance\n",
    "        mean_perf = sum(results[model_name]) / len(results[model_name])\n",
    "        print(f\"Average Performance for {model_name}: {mean_perf}\")\n",
    "\n",
    "        print(f\"{model_name} - Cross-Validation Results: {model_results}\")\n",
    "        print(f\"{model_name} - Mean Performance: {sum(model_results) / len(model_results)}\")\n",
    "        mean_perf_dict[model_name] = mean_perf\n",
    "\n",
    "    return mean_perf_dict"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T18:12:25.133552Z",
     "start_time": "2025-01-22T18:09:35.021822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vgg_path = os.path.join(utils.MODELS_DIR, VGG19)\n",
    "\n",
    "def get_hyperparameters(path):\n",
    "    param_groups = torch.load(path, weights_only=True)['optimizer_state_dict']['param_groups'][0]\n",
    "    return {'lr':param_groups['lr'], 'weight_decay':param_groups['weight_decay']}\n",
    "\n",
    "\n",
    "model1_param_groups = get_hyperparameters(f\"{vgg_path}/best_model_trial_0.pt\") # Load hyperparameters\n",
    "model1_dict = {\n",
    "    'architecture': VGG19,\n",
    "    'param_groups': model1_param_groups\n",
    "}\n",
    "\n",
    "\n",
    "model2_param_groups =  get_hyperparameters(f\"{vgg_path}/best_model_trial_1.pt\") # Load hyperparameters\n",
    "model2_dict = {\n",
    "    'architecture': VGG19,\n",
    "    'param_groups': model2_param_groups\n",
    "}\n",
    "\n",
    "\n",
    "cross_validation(train_dataset, vgg_model1=model1_dict, vgg_model2=model2_dict)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tFold 1\n",
      "Training :vgg_model1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T18:12:27.973935Z",
     "start_time": "2025-01-22T18:12:27.963935Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "source": [
    "# Fine tuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gaSN21pPiGdy",
    "outputId": "8101799f-ef2c-4ada-8a3d-7dc594b2385e",
    "ExecuteTime": {
     "end_time": "2025-01-22T14:01:31.826326Z",
     "start_time": "2025-01-22T14:01:30.798859Z"
    }
   },
   "source": [
    "# Load the AlexNet model \n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)\n",
    "alexnet_model = FinedTunedModel(alexnet, ALEXNET).to(device)\n",
    "alexnet_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinedTunedModel(\n",
       "  (features_extractor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "      (4): ReLU(inplace=True)\n",
       "      (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (7): ReLU(inplace=True)\n",
       "      (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (9): ReLU(inplace=True)\n",
       "      (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T14:02:50.693818Z",
     "start_time": "2025-01-22T14:01:36.162625Z"
    }
   },
   "source": [
    "study = optuna.create_study(study_name=f'{ALEXNET}', direction='minimize')\n",
    "study.optimize(lambda trial: objective(trial, alexnet_model, config={}), n_trials=3)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 16:01:36,164] A new study created in memory with name: AlexNet\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "wandb: Currently logged in as: amitr5 (amitr5-tel-aviv-university). Use `wandb login --relogin` to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\deep_van_gogh\\wandb\\run-20250122_160137-1kactwlt</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/1kactwlt' target=\"_blank\">AlexNet_trial_0</a></strong> to <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/1kactwlt' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/1kactwlt</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr><tr><td>Validation Accuracy</td><td></td></tr><tr><td>Validation Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.99795</td></tr><tr><td>Train Loss</td><td>0.01104</td></tr><tr><td>Validation Accuracy</td><td>0.96854</td></tr><tr><td>Validation Loss</td><td>0.11455</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_trial_0</strong> at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/1kactwlt' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/1kactwlt</a><br> View project at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250122_160137-1kactwlt\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 16:01:57,725] Trial 0 finished with value: 0.10576229227551358 and parameters: {'learning_rate': 8.54475681264745e-05, 'weight_decay': 1.3246324576641997e-06, 'batch_size': 96}. Best is trial 0 with value: 0.10576229227551358.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\deep_van_gogh\\wandb\\run-20250122_160157-0a23wi5w</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/0a23wi5w' target=\"_blank\">AlexNet_trial_1</a></strong> to <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/0a23wi5w' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/0a23wi5w</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr><tr><td>Validation Accuracy</td><td></td></tr><tr><td>Validation Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.98598</td></tr><tr><td>Train Loss</td><td>0.04482</td></tr><tr><td>Validation Accuracy</td><td>0.95896</td></tr><tr><td>Validation Loss</td><td>0.3491</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_trial_1</strong> at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/0a23wi5w' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/0a23wi5w</a><br> View project at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250122_160157-0a23wi5w\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 16:02:24,856] Trial 1 finished with value: 0.12651337678614297 and parameters: {'learning_rate': 0.00035744717416844876, 'weight_decay': 3.63939834492625e-06, 'batch_size': 32}. Best is trial 0 with value: 0.10576229227551358.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Run data is saved locally in <code>C:\\Projects\\deep_van_gogh\\wandb\\run-20250122_160224-ipk3eviz</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/ipk3eviz' target=\"_blank\">AlexNet_trial_2</a></strong> to <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View project at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run at <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/ipk3eviz' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/ipk3eviz</a>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td></td></tr><tr><td>Train Accuracy</td><td></td></tr><tr><td>Train Loss</td><td></td></tr><tr><td>Validation Accuracy</td><td></td></tr><tr><td>Validation Loss</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.98324</td></tr><tr><td>Train Loss</td><td>0.05421</td></tr><tr><td>Validation Accuracy</td><td>0.95896</td></tr><tr><td>Validation Loss</td><td>0.1584</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet_trial_2</strong> at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/ipk3eviz' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh/runs/ipk3eviz</a><br> View project at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250122_160224-ipk3eviz\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-22 16:02:50,670] Trial 2 finished with value: 0.131821774687776 and parameters: {'learning_rate': 0.0008391974354331484, 'weight_decay': 9.707033483493868e-05, 'batch_size': 32}. Best is trial 0 with value: 0.10576229227551358.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-22T11:14:45.934631Z",
     "start_time": "2025-01-22T11:14:45.917633Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gy8lmbPmhj9H",
    "outputId": "330638de-9f78-4d50-c423-bbab448afc37",
    "ExecuteTime": {
     "end_time": "2025-03-23T15:46:26.551820Z",
     "start_time": "2025-03-23T15:46:26.401673Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#define a function to load an image and pre-process it\n",
    "def load_image(img_path, shape=(224,224)):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    # Define transformation to resize, normalize, and convert to tensor\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    # Apply transformations, remove alpha channel, and add batch dimension\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "    return image.to(device)\n",
    "#define a function to extract features from the network\n",
    "def get_features(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram\n",
    "\n",
    "# Deprocess the image by reversing the normalization\n",
    "def deprocess(tensor):\n",
    "    tensor = tensor.cpu().clone().detach()\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return to_pil_image(tensor)\n",
    "\n",
    "\n",
    "def style_transfer(model, style_img_path, content_img_path, content_weight=1, style_weight=1e3, num_steps=5001, model_name='vgg19_pretrained'):\n",
    "    model = model.features #Gives us access to the layers of features\n",
    "\n",
    "    layers = {\n",
    "         '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "         '21': 'conv4_2'\n",
    "    }\n",
    "\n",
    "    style_weights = {\n",
    "        'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3\n",
    "    }\n",
    "\n",
    "    content_layer = 'conv4_2'\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "         param.requires_grad_(False)\n",
    "    # Load and preprocess the content and style images\n",
    "    content = load_image(content_img_path).to(device)\n",
    "    style = load_image(style_img_path).to(device)\n",
    "    # Extract features from content and style images\n",
    "    content_features = get_features(content, model, layers)\n",
    "    style_features = get_features(style, model, layers)\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    for ii in tqdm.tqdm(range(1, num_steps + 1), desc=\"Style Transfer Progress\"):\n",
    "        # Extract features from target image\n",
    "        target_features = get_features(target, model, layers)\n",
    "\n",
    "        # Compute content loss\n",
    "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "        # Compute style loss by comparing Gram matrices for each layer\n",
    "        style_loss = 0\n",
    "        for layer in style_weights:\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        # Calculate total loss and update target image\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track the loss\n",
    "        if ii % 1000 == 0:\n",
    "            losses.append(total_loss.item())\n",
    "            print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    #plt.plot(range(0, len(losses) * 100, 100), losses)\n",
    "    #plt.xlabel('Step')\n",
    "    #plt.ylabel('Total Loss')\n",
    "    #plt.title('Loss during Style Transfer')\n",
    "    #plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "\n",
    "def style_transfer2(model, style_img_path, content_img_path, layers, style_weights, content_layer, content_weight=1, style_weight=1e3, num_steps=5001,shape=(224,224)):\n",
    "    model = model.features  # Gives us access to the layers of features\n",
    "\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    # Load and preprocess the content and style images\n",
    "    content = load_image(content_img_path,shape).to(device)\n",
    "    style = load_image(style_img_path,shape).to(device)\n",
    "    # Extract features from content and style images\n",
    "    content_features = get_features(content, model, layers)\n",
    "    style_features = get_features(style, model, layers)\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    for ii in tqdm.tqdm(range(1, num_steps + 1), desc=\"Style Transfer Progress\"):\n",
    "        # Extract features from target image\n",
    "        target_features = get_features(target, model, layers)\n",
    "\n",
    "        # Compute content loss\n",
    "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "        # Compute style loss by comparing Gram matrices for each layer\n",
    "        style_loss = 0\n",
    "        for layer in style_weights:\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        # Calculate total loss and update target image\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track the loss\n",
    "        if ii % 1000 == 0:\n",
    "            losses.append(total_loss.item())\n",
    "            print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def style_transfer_multi_style(model, style_img_paths, style_weights, content_img_path, layers, style_layer_weights, content_layer, content_weight=1, style_weight=1e3, num_steps=5001, shape=(224,224)):\n",
    "    model = model.features  # Gives us access to the layers of features\n",
    "\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Load and preprocess the content image\n",
    "    content = load_image(content_img_path, shape).to(device)\n",
    "    content_features = get_features(content, model, layers)\n",
    "\n",
    "    # Load and preprocess the style images\n",
    "    style_features_list = []\n",
    "    for style_img_path in style_img_paths:\n",
    "        style = load_image(style_img_path, shape).to(device)\n",
    "        style_features = get_features(style, model, layers)\n",
    "        style_features_list.append(style_features)\n",
    "\n",
    "    # Combine style features by weighted averaging\n",
    "    combined_style_features = {}\n",
    "    for layer in layers.values():\n",
    "        combined_style_features[layer] = torch.zeros_like(style_features_list[0][layer])\n",
    "        for i, style_features in enumerate(style_features_list):\n",
    "            combined_style_features[layer] += style_weights[i] * style_features[layer]\n",
    "        combined_style_features[layer] /= sum(style_weights)\n",
    "\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "    style_grams = {layer: gram_matrix(combined_style_features[layer]) for layer in combined_style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    for ii in tqdm.tqdm(range(1, num_steps + 1), desc=\"Style Transfer Progress\"):\n",
    "        # Extract features from target image\n",
    "        target_features = get_features(target, model, layers)\n",
    "\n",
    "        # Compute content loss\n",
    "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "        # Compute style loss by comparing Gram matrices for each layer\n",
    "        style_loss = 0\n",
    "        for layer in style_layer_weights:\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_layer_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "        # Calculate total loss and update target image\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the loss\n",
    "        if ii % 1000 == 0:\n",
    "            losses.append(total_loss.item())\n",
    "            print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "            # Deprocess and display the intermediate result\n",
    "            intermediate_result = deprocess(target)\n",
    "            plt.imshow(intermediate_result)\n",
    "            plt.title(f\"Step {ii}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "import os\n",
    "\n",
    "# Load the style and content images\n",
    "style_img_paths = [r'C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\\data\\dataset\\Post_Impressionism\\edouard-cortes_theatre-du-chatelet-1.jpg'][0]\n",
    " # style_img_paths = os.path.join(r'C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\\data\\dataset\\Post_Impressionism\\','edouard-cortes_theatre-du-chatelet-1.jpg')\n",
    "\n",
    "\n",
    "content_img_path = r'C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\\data\\dataset\\Post_Impressionism\\edouard-vuillard_autoportrait.jpg'\n",
    "style_weights=1\n",
    "# define the model\n",
    "model = vgg19\n",
    "#define style layers and weights\n",
    "layers = {\n",
    "         '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "         '21': 'conv4_2'\n",
    "    }\n",
    "\n",
    "style_layer_weights = {\n",
    "        'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3\n",
    "    }\n",
    "\n",
    "content_layer = 'conv4_2'\n",
    "# Perform style transfer\n",
    "stylized_image = style_transfer_multi_style(model, style_img_paths=style_img_paths,style_weights=style_weights, content_img_path=content_img_path,style_weight=1e6, content_weight=0,num_steps=5001,layers=layers,style_layer_weights=style_layer_weights,content_layer=content_layer,shape=(224,224))\n",
    "\n",
    "# Deprocess and display the image\n",
    "stylized_image = deprocess(stylized_image)\n",
    "\n",
    "# Load the original content image to get its resolution\n",
    "#original_content_image = Image.open(content_img_path)\n",
    "#original_resolution = original_content_image.size\n",
    "\n",
    "# Resize the stylized image to the original resolution\n",
    "#stylized_image = stylized_image.resize(original_resolution, Image.LANCZOS)\n",
    "\n",
    "# Display the upscaled image\n",
    "stylized_image.show()\n",
    "output_path = 'output.jpg'\n",
    "stylized_image.save(output_path)\n",
    "\n"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\amitr5\\\\PycharmProjects\\\\deep_van_gogh\\\\C'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 251\u001B[0m\n\u001B[0;32m    249\u001B[0m content_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconv4_2\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[0;32m    250\u001B[0m \u001B[38;5;66;03m# Perform style transfer\u001B[39;00m\n\u001B[1;32m--> 251\u001B[0m stylized_image \u001B[38;5;241m=\u001B[39m \u001B[43mstyle_transfer_multi_style\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstyle_img_paths\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstyle_img_paths\u001B[49m\u001B[43m,\u001B[49m\u001B[43mstyle_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstyle_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent_img_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent_img_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43mstyle_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1e6\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontent_weight\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mnum_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5001\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43mlayers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m,\u001B[49m\u001B[43mstyle_layer_weights\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstyle_layer_weights\u001B[49m\u001B[43m,\u001B[49m\u001B[43mcontent_layer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontent_layer\u001B[49m\u001B[43m,\u001B[49m\u001B[43mshape\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m224\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    253\u001B[0m \u001B[38;5;66;03m# Deprocess and display the image\u001B[39;00m\n\u001B[0;32m    254\u001B[0m stylized_image \u001B[38;5;241m=\u001B[39m deprocess(stylized_image)\n",
      "Cell \u001B[1;32mIn[13], line 172\u001B[0m, in \u001B[0;36mstyle_transfer_multi_style\u001B[1;34m(model, style_img_paths, style_weights, content_img_path, layers, style_layer_weights, content_layer, content_weight, style_weight, num_steps, shape)\u001B[0m\n\u001B[0;32m    170\u001B[0m style_features_list \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    171\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m style_img_path \u001B[38;5;129;01min\u001B[39;00m style_img_paths:\n\u001B[1;32m--> 172\u001B[0m     style \u001B[38;5;241m=\u001B[39m \u001B[43mload_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstyle_img_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mshape\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m    173\u001B[0m     style_features \u001B[38;5;241m=\u001B[39m get_features(style, model, layers)\n\u001B[0;32m    174\u001B[0m     style_features_list\u001B[38;5;241m.\u001B[39mappend(style_features)\n",
      "Cell \u001B[1;32mIn[13], line 5\u001B[0m, in \u001B[0;36mload_image\u001B[1;34m(img_path, shape)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload_image\u001B[39m(img_path, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m224\u001B[39m,\u001B[38;5;241m224\u001B[39m)):\n\u001B[1;32m----> 5\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mImage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg_path\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mconvert(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mRGB\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      6\u001B[0m     \u001B[38;5;66;03m# Define transformation to resize, normalize, and convert to tensor\u001B[39;00m\n\u001B[0;32m      7\u001B[0m     in_transform \u001B[38;5;241m=\u001B[39m transforms\u001B[38;5;241m.\u001B[39mCompose([\n\u001B[0;32m      8\u001B[0m         transforms\u001B[38;5;241m.\u001B[39mResize(shape),\n\u001B[0;32m      9\u001B[0m         transforms\u001B[38;5;241m.\u001B[39mToTensor(),\n\u001B[0;32m     10\u001B[0m         transforms\u001B[38;5;241m.\u001B[39mNormalize((\u001B[38;5;241m0.485\u001B[39m, \u001B[38;5;241m0.456\u001B[39m, \u001B[38;5;241m0.406\u001B[39m), (\u001B[38;5;241m0.229\u001B[39m, \u001B[38;5;241m0.224\u001B[39m, \u001B[38;5;241m0.225\u001B[39m))\n\u001B[0;32m     11\u001B[0m     ])\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\PIL\\Image.py:3431\u001B[0m, in \u001B[0;36mopen\u001B[1;34m(fp, mode, formats)\u001B[0m\n\u001B[0;32m   3428\u001B[0m     filename \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mrealpath(os\u001B[38;5;241m.\u001B[39mfspath(fp))\n\u001B[0;32m   3430\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m filename:\n\u001B[1;32m-> 3431\u001B[0m     fp \u001B[38;5;241m=\u001B[39m \u001B[43mbuiltins\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m   3432\u001B[0m     exclusive_fp \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m   3433\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\amitr5\\\\PycharmProjects\\\\deep_van_gogh\\\\C'"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "2cMigqU0CGg5",
    "outputId": "afdc3a89-f2ad-457b-eeb7-b4aabcf214ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2UNqmGA2DXDj",
    "outputId": "18e5c745-d27d-4f23-97cf-717b194bd3c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dC9MS7V6WZF",
    "outputId": "80757501-0728-4fe1-96b7-a15122faad92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w93PNhRgHVue",
    "outputId": "704e1556-8d2f-4c90-af0e-bc26aa7a072e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24UKrxg-H25c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
