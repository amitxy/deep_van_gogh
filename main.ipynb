{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:43:59.223490Z",
     "start_time": "2025-01-13T18:43:53.089926Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports:\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import VGG19_Weights\n",
    "\n",
    "# Project utilities:\n",
    "import utils\n",
    "import preprocessing\n",
    "import optuna\n",
    "import wandb"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TEMP.TAU.000\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:00.956320Z",
     "start_time": "2025-01-13T18:44:00.940032Z"
    }
   },
   "source": [
    "# Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms = True\n",
    "\n",
    "\n",
    "ROOT = './data'\n",
    "DATASET_DIR = './data/Post_Impressionism'\n",
    "CSV_PATH = './data/classes.csv'\n",
    "\n",
    "if not (os.path.exists(DATASET_DIR) and os.path.exists(CSV_PATH)):\n",
    "    raise FileExistsError(\"File doesn't exist, we expect a data folder with the labels and the images folder as in the lab\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:02.620071Z",
     "start_time": "2025-01-13T18:44:02.569876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:04.298595Z",
     "start_time": "2025-01-13T18:44:04.072421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224, 0.225])]) # Normalize the pixel values based on ImageNet statistics, to a range that VGG expects\n",
    "\n",
    "dataset = preprocessing.ImageFolderForBinaryClassification(root=ROOT, target='is_van_gogh', transform=transform)\n",
    "# Take only pictures that are in inside the csv\n",
    "pics_in_csv = [i for i in range(len(dataset)) if dataset.samples[i][1] >= 0]\n",
    "dataset = Subset(dataset, pics_in_csv)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:06.185760Z",
     "start_time": "2025-01-13T18:44:06.147897Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = pd.read_csv(CSV_PATH)\n",
    "train_classes = classes[classes['subset'] == 'train']\n",
    "test_classes = classes[classes['subset'] == 'test']\n",
    "\n",
    "train_indices, val_indices = train_test_split(train_classes.index.to_list(), test_size=0.2, random_state=SEED)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "classes.head()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                            filename            artist  \\\n",
       "0  Post_Impressionism/edouard-cortes_the-theater-...    edouard cortes   \n",
       "1  Post_Impressionism/edouard-cortes_theatre-du-c...    edouard cortes   \n",
       "2  Post_Impressionism/edouard-vuillard_boulevard-...  edouard vuillard   \n",
       "3  Post_Impressionism/edouard-vuillard_figures-ea...  edouard vuillard   \n",
       "4  Post_Impressionism/edouard-vuillard_sacha-guit...  edouard vuillard   \n",
       "\n",
       "                    genre                              description  \\\n",
       "0  ['Post Impressionism']     the-theater-of-the-comedie-francaise   \n",
       "1  ['Post Impressionism']                    theatre-du-chatelet-1   \n",
       "2  ['Post Impressionism']                boulevard-of-battignolles   \n",
       "3  ['Post Impressionism']  figures-eating-in-a-garden-by-the-water   \n",
       "4  ['Post Impressionism']   sacha-guitry-in-his-dressing-room-1912   \n",
       "\n",
       "              phash  width  height  genre_count subset  is_van_gogh  \n",
       "0  9491ada9caf05cf1   1675    1382            1  train            0  \n",
       "1  c7d69030996f36e4   1896    1382            1  train            0  \n",
       "2  eb7214d866c638b5   1688    1382            1  train            0  \n",
       "3  d3272568d0a95e3d   1684    1382            1  train            0  \n",
       "4  dae0254af31a6fa4   1818    1382            1  train            0  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>description</th>\n",
       "      <th>phash</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>genre_count</th>\n",
       "      <th>subset</th>\n",
       "      <th>is_van_gogh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Post_Impressionism/edouard-cortes_the-theater-...</td>\n",
       "      <td>edouard cortes</td>\n",
       "      <td>['Post Impressionism']</td>\n",
       "      <td>the-theater-of-the-comedie-francaise</td>\n",
       "      <td>9491ada9caf05cf1</td>\n",
       "      <td>1675</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Post_Impressionism/edouard-cortes_theatre-du-c...</td>\n",
       "      <td>edouard cortes</td>\n",
       "      <td>['Post Impressionism']</td>\n",
       "      <td>theatre-du-chatelet-1</td>\n",
       "      <td>c7d69030996f36e4</td>\n",
       "      <td>1896</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Post_Impressionism/edouard-vuillard_boulevard-...</td>\n",
       "      <td>edouard vuillard</td>\n",
       "      <td>['Post Impressionism']</td>\n",
       "      <td>boulevard-of-battignolles</td>\n",
       "      <td>eb7214d866c638b5</td>\n",
       "      <td>1688</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Post_Impressionism/edouard-vuillard_figures-ea...</td>\n",
       "      <td>edouard vuillard</td>\n",
       "      <td>['Post Impressionism']</td>\n",
       "      <td>figures-eating-in-a-garden-by-the-water</td>\n",
       "      <td>d3272568d0a95e3d</td>\n",
       "      <td>1684</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Post_Impressionism/edouard-vuillard_sacha-guit...</td>\n",
       "      <td>edouard vuillard</td>\n",
       "      <td>['Post Impressionism']</td>\n",
       "      <td>sacha-guitry-in-his-dressing-room-1912</td>\n",
       "      <td>dae0254af31a6fa4</td>\n",
       "      <td>1818</td>\n",
       "      <td>1382</td>\n",
       "      <td>1</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:09.464839Z",
     "start_time": "2025-01-13T18:44:09.250421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "n_times = 25\n",
    "dropout_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    *([transforms.RandomErasing(p=0.5, scale=(0.01, 0.01), ratio=(1, 1))]*n_times),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224, 0.225])\n",
    "])\n",
    "dropout_dataset = Subset(preprocessing.ImageFolderForBinaryClassification(root=ROOT, transform=dropout_transform, target='is_van_gogh'), train_indices)\n",
    "augmented_train_dataset = ConcatDataset([dropout_dataset, train_dataset])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:13.695173Z",
     "start_time": "2025-01-13T18:44:13.676279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(augmented_train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "source": [
    "# Fine tuning VGG19"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ug6-1DB33fkH",
    "outputId": "03123ea9-33d7-447f-ad37-ed54df78747e",
    "ExecuteTime": {
     "end_time": "2025-01-13T18:44:30.824973Z",
     "start_time": "2025-01-13T18:44:29.320940Z"
    }
   },
   "source": [
    "# Load pre-trained VGG19 model\n",
    "# vgg19 = models.vgg19(pretrained=True)\n",
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device)\n",
    "vgg19"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T18:47:51.357703Z",
     "start_time": "2025-01-13T18:47:51.335252Z"
    }
   },
   "source": [
    "from train import train_model_with_hyperparams\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3) #Suggests a learning_rate value from a log-uniform distribution between 1e-5 and 1e-3 for hyperparameter optimization using Optuna.\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4) # Suggests a weight_decay value from a log-uniform distribution between 1e-6 and 1e-4 for regularization during Optuna hyperparameter optimization.\n",
    "    patience = trial.suggest_int(\"patience\", 3, 10) #I don't really like putting the patience as an hyper parameter - this is a thing that needs to be determined according to constraints. I put it here just to show that this is possible.\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16) # Basically choosing between 16,32,64\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Load the train DataLoader with the chosen batch_size\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Load the val DataLoader with the chosen batch_size\n",
    "\n",
    "    # Load the pre-trained model VGG16\n",
    "    model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device) # pretrained=True == with the trained weights\n",
    "\n",
    "    # Freeze layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "    # Unfreeze the last 6 layers in the features part of the VGG\n",
    "    for param in model.features[-6:].parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Modify the classifier to fit our problem (10 classes)\n",
    "    model.classifier[6] = nn.Linear(4096, 10) # Replaces the final layer of the VGG16 classifier with a new fully connected (nn.Linear) layer. The new wights are initialized randomly and hence will need to be trained\n",
    "    model.classifier[6] = model.classifier[6].to(device)\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss() # Classification.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Adam, like always, with the chosen parameters from Optuna\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=\"project-vgg19\", #init == set the project and the \"general\" parameters\n",
    "               config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"architecture\": \"VGG19\",\n",
    "        \"dataset\": \"Post_Imp-10\"\n",
    "    },\n",
    "    name=f\"trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best validation loss\n",
    "    best_val_loss = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=20, patience=patience, trial=trial, device = device) #send this trinal to the function above\n",
    "\n",
    "    # Finish the Weights & Biases run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return best_val_loss"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-13T18:48:43.821500Z"
    }
   },
   "source": [
    "# Define and optimize the Optuna study\n",
    "study = optuna.create_study(direction = \"minimize\") # Specifies that the goal of the optimization is to minimize the objective function (e.g., validation loss because this is what returned from the objective).\n",
    "study.optimize(objective, n_trials = 10) # 10 trials"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-13 20:48:43,824] A new study created in memory with name: no-name-22234ee5-ede5-474d-a79f-4131268a2588\n",
      "C:\\Users\\TEMP.TAU.000\\AppData\\Local\\Temp\\ipykernel_20408\\1195001642.py:6: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3) #Suggests a learning_rate value from a log-uniform distribution between 1e-5 and 1e-3 for hyperparameter optimization using Optuna.\n",
      "C:\\Users\\TEMP.TAU.000\\AppData\\Local\\Temp\\ipykernel_20408\\1195001642.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4) # Suggests a weight_decay value from a log-uniform distribution between 1e-6 and 1e-4 for regularization during Optuna hyperparameter optimization.\n",
      "C:\\Users\\TEMP.TAU.000\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\TEMP.TAU.000\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "source": [
    "# Fine tuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gaSN21pPiGdy",
    "outputId": "8101799f-ef2c-4ada-8a3d-7dc594b2385e"
   },
   "outputs": [],
   "source": [
    "# Load the AlexNet model \n",
    "alexnet = models.alexnet(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LRQUj_gU93MT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "yhlrhEFw4Vm6",
    "outputId": "278d8206-40e6-4266-a1f8-38c87411a3f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gy8lmbPmhj9H",
    "outputId": "330638de-9f78-4d50-c423-bbab448afc37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "2cMigqU0CGg5",
    "outputId": "afdc3a89-f2ad-457b-eeb7-b4aabcf214ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2UNqmGA2DXDj",
    "outputId": "18e5c745-d27d-4f23-97cf-717b194bd3c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dC9MS7V6WZF",
    "outputId": "80757501-0728-4fe1-96b7-a15122faad92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w93PNhRgHVue",
    "outputId": "704e1556-8d2f-4c90-af0e-bc26aa7a072e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24UKrxg-H25c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
