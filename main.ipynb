{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T11:40:03.234169Z",
     "start_time": "2025-01-14T11:40:03.209255Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from optuna import trial\n",
    "from sentry_sdk.utils import epoch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision.models import VGG19_Weights\n",
    "from torchvision.transforms.v2 import Transform\n",
    "\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# ***CHANGE to your directory if not working locally***\n",
    "# DIR = r'C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh'\n",
    "DIR = r'E:\\My Drive\\Intro to deep learning\\deep_van_gogh'\n",
    "os.chdir(DIR)\n",
    "print(\"Current Working Directory:\", os.getcwd())\n",
    "\n",
    "# Project utilities\n",
    "import utils\n",
    "import preprocessing\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms = True\n",
    "\n",
    "ROOT = './data/dataset'\n",
    "DATASET_DIR = 'data/dataset/Post_Impressionism'\n",
    "CSV_PATH = 'data/dataset/classes.csv'\n",
    "OPTIMIZED_DIR = './data/optimized'\n",
    "\n",
    "if not (os.path.exists(DATASET_DIR) and os.path.exists(CSV_PATH)):\n",
    "    raise FileExistsError(\"File doesn't exist, we expect a data folder with the labels and the images folder as in the lab\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: E:\\My Drive\\Intro to deep learning\\deep_van_gogh\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T11:58:55.574833Z",
     "start_time": "2025-01-14T11:58:39.028112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(), # Convert the image to a PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224, 0.225])]) # Normalize the pixel values based on ImageNet statistics, to a range that VGG expects\n",
    "\n",
    "optimized_dataset_path = OPTIMIZED_DIR + '/dataset.npz'\n",
    "\n",
    "if os.path.exists(optimized_dataset_path):\n",
    "    dataset = preprocessing.NumPyDataset('data/optimized/dataset.npz')\n",
    "    pics_in_csv = {i for i in range(len(dataset)) if dataset.labels[i] >= 0}\n",
    "else:\n",
    "    dataset = preprocessing.ImageFolderForBinaryClassification(root=ROOT, target='is_van_gogh', transform=transform)\n",
    "    pics_in_csv = {i for i in range(len(dataset)) if dataset.samples[i][1] >= 0}\n",
    "\n",
    "# Take only pictures that are in inside the csv\n",
    "classes = pd.read_csv(CSV_PATH)\n",
    "train_classes = list(pics_in_csv.intersection(classes[classes['subset'] == 'train'].index.tolist()))\n",
    "test_classes = list(pics_in_csv.intersection(classes[classes['subset'] == 'test'].index.tolist()))"
   ],
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T11:58:58.751100Z",
     "start_time": "2025-01-14T11:58:58.722049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_indices, val_indices = train_test_split(list(train_classes), test_size=0.2, random_state=SEED)\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)"
   ],
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:07:16.417404Z",
     "start_time": "2025-01-14T12:07:16.030262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(SEED)\n",
    "n_times = 25\n",
    "dropout_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    *([transforms.RandomErasing(p=0.5, scale=(0.01, 0.01), ratio=(1, 1))]*n_times),\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224, 0.225])\n",
    "])\n",
    "dropout_dataset = Subset(preprocessing.ImageFolderForBinaryClassification(root=ROOT, transform=dropout_transform, target='is_van_gogh'), train_indices)\n",
    "# augmented_train_dataset = ConcatDataset([dropout_dataset, train_dataset])\n",
    "augmented_train_dataset = train_dataset\n",
    "# train_loader = DataLoader(augmented_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=8)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "source": [
    "# Fine tuning VGG19"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ug6-1DB33fkH",
    "outputId": "03123ea9-33d7-447f-ad37-ed54df78747e",
    "ExecuteTime": {
     "end_time": "2025-01-14T12:03:30.601435Z",
     "start_time": "2025-01-14T12:03:29.024436Z"
    }
   },
   "source": [
    "# Load pre-trained VGG19 model\n",
    "vgg_model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device)\n",
    "for param in vgg_model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "vgg_model.classifier[-1] = nn.Linear(4096, 2).to(device)\n",
    "vgg_model\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:07:22.180881Z",
     "start_time": "2025-01-14T12:07:22.168202Z"
    }
   },
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from train import train_model_with_hyperparams\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial, model):\n",
    "    # Hyperparameter suggestions\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3) #Suggests a learning_rate value from a log-uniform distribution between 1e-5 and 1e-3 for hyperparameter optimization using Optuna.\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4) # Suggests a weight_decay value from a log-uniform distribution between 1e-6 and 1e-4 for regularization during Optuna hyperparameter optimization.\n",
    "    patience = trial.suggest_int(\"patience\", 3, 10) #I don't really like putting the patience as a hyperparameter - this is a thing that needs to be determined according to constraints.\n",
    "    batch_size = trial.suggest_int(\"batch_size\", 16, 64, step=16) # Basically choosing between 16,32,64\n",
    "\n",
    "    train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True) # Load the train DataLoader with the chosen batch_size\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Load the val DataLoader with the chosen batch_size\n",
    "\n",
    "    # Load the pre-trained model VGG16\n",
    "    # model = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device)\n",
    "\n",
    "    # Freeze layers\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Unfreeze the last 6 layers in the features part of the VGG\n",
    "    # for param in model.features[-6:].parameters():\n",
    "    #     param.requires_grad = True\n",
    "\n",
    "    # Modify the classifier to fit our problem (10 classes)\n",
    "    model.classifier[-1] = nn.Linear(4096, 2).to(device) # Replaces the final layer of the VGG16 classifier with a new fully connected (nn.Linear) layer. The new wights are initialized\n",
    "    # Define optimizer and loss function\n",
    "    criterion = nn.CrossEntropyLoss() # Classification.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay) # Adam, like always, with the chosen parameters from Optuna\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    wandb.init(project=\"deep_van_gogh-vgg19\", #init == set the project and the \"general\" parameters\n",
    "               config={\n",
    "        \"learning_rate\": learning_rate,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"architecture\": \"VGG19\",\n",
    "        \"dataset\": \"Post_Imp-10\"\n",
    "    },\n",
    "    name=f\"trial_{trial.number}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best validation loss\n",
    "    epochs = 5\n",
    "    best_val_loss = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=epochs, patience=patience, trial=trial, device=device) #send this trinal to the function above\n",
    "\n",
    "    # Finish the Weights & Biases run\n",
    "    wandb.finish()\n",
    "\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return best_val_loss"
   ],
   "outputs": [],
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-14T12:08:56.781299Z",
     "start_time": "2025-01-14T12:07:26.303911Z"
    }
   },
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(lambda trial : objective(trial, vgg_model), n_trials=1, )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 14:07:26,307] A new study created in memory with name: no-name-d922a91e-1a0e-4bdb-81dd-8411df0cb0c0\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_27776\\202831435.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-5, 1e-3) #Suggests a learning_rate value from a log-uniform distribution between 1e-5 and 1e-3 for hyperparameter optimization using Optuna.\n",
      "C:\\Users\\Amit\\AppData\\Local\\Temp\\ipykernel_27776\\202831435.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-4) # Suggests a weight_decay value from a log-uniform distribution between 1e-6 and 1e-4 for regularization during Optuna hyperparameter optimization.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▃▅▆█</td></tr><tr><td>Train Accuracy</td><td>▁▃▆██</td></tr><tr><td>Train Loss</td><td>█▅▂▁▁</td></tr><tr><td>Validation Accuracy</td><td>▁▆▅█▇</td></tr><tr><td>Validation Loss</td><td>▅▁▅██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>5</td></tr><tr><td>Train Accuracy</td><td>0.99956</td></tr><tr><td>Train Loss</td><td>0.00141</td></tr><tr><td>Validation Accuracy</td><td>0.97378</td></tr><tr><td>Validation Loss</td><td>0.18514</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">trial_0</strong> at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-vgg19/runs/oh4ijuom' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-vgg19/runs/oh4ijuom</a><br> View project at: <a href='https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-vgg19' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/deep_van_gogh-vgg19</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250114_140422-oh4ijuom\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-14 14:08:56,766] Trial 0 finished with value: 0.09964846864251511 and parameters: {'learning_rate': 7.732642367785799e-05, 'weight_decay': 9.214387766525893e-06, 'patience': 7, 'batch_size': 16}. Best is trial 0 with value: 0.09964846864251511.\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "source": [
    "# Fine tuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gaSN21pPiGdy",
    "outputId": "8101799f-ef2c-4ada-8a3d-7dc594b2385e"
   },
   "outputs": [],
   "source": [
    "# Load the AlexNet model \n",
    "alexnet = models.alexnet(pretrained=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LRQUj_gU93MT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "id": "yhlrhEFw4Vm6",
    "outputId": "278d8206-40e6-4266-a1f8-38c87411a3f7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gy8lmbPmhj9H",
    "outputId": "330638de-9f78-4d50-c423-bbab448afc37"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "2cMigqU0CGg5",
    "outputId": "afdc3a89-f2ad-457b-eeb7-b4aabcf214ca"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2UNqmGA2DXDj",
    "outputId": "18e5c745-d27d-4f23-97cf-717b194bd3c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dC9MS7V6WZF",
    "outputId": "80757501-0728-4fe1-96b7-a15122faad92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w93PNhRgHVue",
    "outputId": "704e1556-8d2f-4c90-af0e-bc26aa7a072e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24UKrxg-H25c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
