{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:46:54.644296Z",
     "start_time": "2025-03-30T15:46:49.505928Z"
    }
   },
   "source": [
    "# All relevant imports:\n",
    "import os\n",
    "import random\n",
    "#from platform import architecture\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.f2py.cfuncs import includes\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sympy.codegen import Print\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from collections import defaultdict\n",
    "from optuna import trial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import optuna\n",
    "import wandb\n",
    "# Project utilities\n",
    "import utils\n",
    "from train import train_model_with_hyperparams\n",
    "\n",
    "VGG19 = 'VGG19'\n",
    "ALEXNET = 'AlexNet'\n",
    "\n",
    "# Set seed\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "# torch.backends.cudnn.deterministic = True\n",
    "# torch.use_deterministic_algorithms = True"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghefter\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:17.425967Z",
     "start_time": "2025-03-30T15:47:17.371252Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setting the device to be GPU if possible (as we learned to do throughout the course):\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "# Check if you're working locally or not\n",
    "if not (os.path.exists(utils.CSV_PATH) and os.path.exists(utils.OPTIMIZED_DIR)):\n",
    "    print(f\"[!] You are NOT on the project's directory [!]\\n\"\n",
    "          f\"Please run the following command (in either CMD or anaconda prompt): \\n\"\n",
    "          f\"jupyter notebook --notebook-dir PROJECT_DIR\\n\"\n",
    "          r\"Where PROJECT_DIR is the project's directory in your computer e.g: C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "[!] You are NOT on the project's directory [!]\n",
      "Please run the following command (in either CMD or anaconda prompt): \n",
      "jupyter notebook --notebook-dir PROJECT_DIR\n",
      "Where PROJECT_DIR is the project's directory in your computer e.g: C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading our data\n",
    "We will load the optimized datasets from our custom dataset object\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:22.829857Z",
     "start_time": "2025-03-30T15:47:22.110983Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumPyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.images = data[\"images\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, 'dataset.npz'))"
   ],
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/optimized/dataset.npz'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m         y \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels[idx], dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mlong)\n\u001B[0;32m     13\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m x, y\n\u001B[1;32m---> 15\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mNumPyDataset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mjoin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mOPTIMIZED_DIR\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdataset.npz\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m, in \u001B[0;36mNumPyDataset.__init__\u001B[1;34m(self, file_path)\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\u001B[38;5;28mself\u001B[39m, file_path):\n\u001B[1;32m----> 3\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mimages \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimages\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m      5\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlabels \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\numpy\\lib\\npyio.py:405\u001B[0m, in \u001B[0;36mload\u001B[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001B[0m\n\u001B[0;32m    403\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    404\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 405\u001B[0m     fid \u001B[38;5;241m=\u001B[39m stack\u001B[38;5;241m.\u001B[39menter_context(\u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mos_fspath\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m)\n\u001B[0;32m    406\u001B[0m     own_fid \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    408\u001B[0m \u001B[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001B[39;00m\n",
      "\u001B[1;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: './data/optimized/dataset.npz'"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can find the optimized dataset files <a href=\"https://drive.google.com/drive/folders/1TBlNcRsRHJ7_rxh_h7_yn_-Ak66Uj_mp?usp=sharing\">HERE</a><br/>\n",
    "Loading the train and test datasets:"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "classes = pd.read_csv(utils.CSV_PATH)\n",
    "# train_indices, val_indices = train_test_split(classes[classes['subset'] == 'train'].index.tolist(), test_size=0.2, random_state=SEED)\n",
    "train_indices = classes[classes['subset'] != 'test'].index.tolist()\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, classes[classes['subset'] == 'test'].index.tolist())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_opt_dataset(dataset_name, indices=None):\n",
    "    ds = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, f'{dataset_name}.npz'))\n",
    "    if indices:\n",
    "        ds = Subset(ds, indices)\n",
    "    return ds\n",
    " # Applying various transformations - flip, dropout, affine, blur, on the dataset assigned for training:\n",
    "flip_dataset = get_opt_dataset('flip', train_indices)\n",
    "dropout_dataset = get_opt_dataset('dropout',train_indices)\n",
    "affine_dataset = get_opt_dataset('affine', train_indices)\n",
    "blur_dataset = get_opt_dataset('blur')\n",
    "\n",
    "augmented_train_dataset = ConcatDataset([train_dataset, flip_dataset, dropout_dataset, affine_dataset])\n",
    "# augmented_train_dataset = train_dataset\n",
    "# train_loader = DataLoader(augmented_train_dataset, batch_size=128, shuffle=True, num_workers=4, pin_memory=True, prefetch_factor=8)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=128, shuffle=False, num_workers=4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "source": [
    "# Fine tuning VGG19"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:29.339625Z",
     "start_time": "2025-03-30T15:47:29.331108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FinedTunedModel(nn.Module):\n",
    "    def __init__(self, base_model, architecture:str):\n",
    "        super(FinedTunedModel, self).__init__()\n",
    "        self._architecture = architecture  # Save the base model architecture\n",
    "        base_children_list = list(base_model.children())\n",
    "        self.features_extractor = nn.Sequential(*base_children_list[:-1]).to(device)\n",
    "        for param in self.features_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to fit to our problem (2 classes)\n",
    "        self.classifier = nn.Sequential(*base_children_list[-1])\n",
    "        self.classifier[-1] = nn.Linear(4096, 2).to(device)  # Replaces the final layer of the base model's classifier with a new fully connected layer\n",
    "        #self.classifier = nn.Linear(4096, 2).to(device)  # Replaces the entire base model's classifier with a new fully connected layer - only if time permits\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_model_output = self.features_extractor(x)\n",
    "        return self.classifier(torch.flatten(base_model_output, start_dim=1))\n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self._architecture\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:33.690235Z",
     "start_time": "2025-03-30T15:47:32.327075Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device) # Load pre-trained VGG19 model\n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device) # Load pre-trained AlexNet model\n",
    "vgg_model = FinedTunedModel(vgg19, VGG19).to(device)\n",
    "vgg_model"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FinedTunedModel(\n",
       "  (features_extractor): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (1): ReLU(inplace=True)\n",
       "      (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (3): ReLU(inplace=True)\n",
       "      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (6): ReLU(inplace=True)\n",
       "      (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (8): ReLU(inplace=True)\n",
       "      (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (11): ReLU(inplace=True)\n",
       "      (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (13): ReLU(inplace=True)\n",
       "      (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (15): ReLU(inplace=True)\n",
       "      (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (17): ReLU(inplace=True)\n",
       "      (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (20): ReLU(inplace=True)\n",
       "      (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (22): ReLU(inplace=True)\n",
       "      (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (24): ReLU(inplace=True)\n",
       "      (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (26): ReLU(inplace=True)\n",
       "      (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (29): ReLU(inplace=True)\n",
       "      (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (31): ReLU(inplace=True)\n",
       "      (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (33): ReLU(inplace=True)\n",
       "      (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "      (35): ReLU(inplace=True)\n",
       "      (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    )\n",
       "    (1): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the image preprocessing steps - resize, center crop, convert to tensor, and normalize to match the pre-trained models' (AlexNet & VGG-19) input requirements from ImageNet:\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to load and preprocess the image\n",
    "def load_and_preprocess_image(img_path):\n",
    "    input_image = Image.open(img_path).convert('RGB')\n",
    "    input_tensor = preprocess(input_image)\n",
    "    input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model\n",
    "    return input_batch\n",
    "\n",
    "# Function to activate the model on the image and print the result\n",
    "def activate_model_on_image(model, img_path, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    input_batch = load_and_preprocess_image(img_path).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_batch)\n",
    "\n",
    "    # Print the raw output\n",
    "    print(\"Model output:\", output)\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "img_path = r'D:\\Users\\kanatcohen\\PycharmProjects\\deep_van_gogh\\data\\dataset\\Post_Impressionism\\vincent-van-gogh_garden-with-flowers-1888-1(1).jpg'\n",
    "output=activate_model_on_image(vgg_model, img_path, device)\n",
    "print(output[0,0])\n",
    "print(torch.max(output,0))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_validation(dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial, num_layers_finetune):\n",
    "    # Initialize KFold - with num_folds = 4, as instructed:\n",
    "    k_folds = 4\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Track performance for each model\n",
    "    results = defaultdict(list)\n",
    "    base_model = vgg19 if architecture == VGG19 else alexnet\n",
    "\n",
    "    best_values = []\n",
    "    # indices = dataset.indices\n",
    "    # labels = dataset.dataset.labels[indices]\n",
    "    indices = np.arange(len(dataset))\n",
    "    labels = np.tile(train_dataset.dataset.labels[train_dataset.indices],len(dataset.datasets))\n",
    "    # Iterating over the folds:\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(indices, labels)):\n",
    "        ######### Weights & Biases Initialization Per Trial & Fold:\n",
    "        wandb.init(project=f'{architecture} - deep_van_gogh - aug - HP Tuning - 27.3.2025',\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": 128,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{architecture}_trial_{trial.number + 1}_fold_{fold}\")\n",
    "        ################\n",
    "\n",
    "        # Subset the dataset for this fold:\n",
    "        train_subset = Subset(dataset, train_ids)\n",
    "        val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "        # Create data loaders - train & validation:\n",
    "        train_loader = DataLoader(train_subset, batch_size=128, pin_memory=True, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=128, pin_memory=True, shuffle=False)\n",
    "        model = FinedTunedModel(base_model.to(device), architecture).to(device)\n",
    "        for param in model.features_extractor[-num_layers_finetune:].parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Train the model\n",
    "        best_val = train_model_with_hyperparams(model,\n",
    "                                                train_loader,\n",
    "                                                val_loader,\n",
    "                                                optimizer,\n",
    "                                                criterion,\n",
    "                                                epochs=epochs,\n",
    "                                                patience=patience,\n",
    "                                                device=device,\n",
    "                                                trial=trial,\n",
    "                                                architecture=architecture, fold=fold)\n",
    "        wandb.log({'Fold': fold, 'AUC':best_val})\n",
    "        best_values.append(best_val)\n",
    "        # Finish the Weights & Biases run\n",
    "        wandb.finish()\n",
    "\n",
    "    return np.mean(best_values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "wandb.finish()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Optuna for our vgg model with the default config\n",
    "study = optuna.create_study(study_name=VGG19, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, vgg_model, config={}), n_trials=15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optuna objective function\n",
    "def objective(trial, model, config: dict) -> float:\n",
    "    \"\"\"\n",
    "    Generic Optuna objective function.\n",
    "    :param trial: Optuna trial object.\n",
    "    :param model: The neural network model to train\n",
    "    :param config: A dictionary with configurable values such as learning rate ranges, batch size ranges, etc.\n",
    "    :return:  best_val_loss: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions based on config - for LR, Weight Decay & Number of Epochs:\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\",\n",
    "                                        config.get(\"lr_min\", 1e-5),\n",
    "                                        config.get(\"lr_max\", 1e-3),\n",
    "                                        log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\",\n",
    "                                       config.get(\"wd_min\", 1e-6),\n",
    "                                       config.get(\"wd_max\", 1e-4),\n",
    "                                       log=True)\n",
    "    # batch_size = trial.suggest_int(\"batch_size\",\n",
    "    #                                config.get(\"batch_size_min\", 32),\n",
    "    #                                config.get(\"batch_size_max\", 128),\n",
    "    #                                step=config.get(\"batch_size_step\", 16))\n",
    "    epochs = trial.suggest_int(\"epochs\", config.get(\"epochs_min\", 10), config.get(\"epochs_max\",30))\n",
    "    patience = config.get(\"patience\", 8)\n",
    "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3) # Including the option not to perform fine-tuning, or only a small num of layers within the feature extractor.\n",
    "\n",
    "    # train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True) # Load the train DataLoader with the chosen batch_size\n",
    "    # val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False) # Load the val DataLoader with the chosen batch_size\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = config.get(\"criterion\", nn.CrossEntropyLoss()) # Classification.\n",
    "\n",
    "    # optimizer_class = config.get(\"optimizer_class\", optim.Adam)\n",
    "    # optimizer = optimizer_class(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Initialize Weights & Biases - the values in the config are the properties of each trial.\n",
    "    architecture = getattr(model, \"architecture\", model.__class__.__name__)\n",
    "    # wandb.init(project=\"deep_van_gogh\",\n",
    "    #            config={\n",
    "    #     \"learning_rate\": learning_rate,\n",
    "    #     \"weight_decay\": weight_decay,\n",
    "    #     \"patience\": patience,\n",
    "    #     \"batch_size\": batch_size,\n",
    "    #     \"epochs\": epochs,\n",
    "    #     \"architecture\": architecture,\n",
    "    #     \"dataset\": \"Post_Impressionism\"\n",
    "    # },\n",
    "    # name=f\"{architecture}_trial_{trial.number+1}\") # The name that will be saved in the W&B platform\n",
    "\n",
    "    # Train the model and get the best val_auc\n",
    "    # mean_val = cross_validation(train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial)\n",
    "    mean_val = cross_validation(augmented_train_dataset, learning_rate, weight_decay, criterion, epochs, patience, device, architecture, trial, num_layers_finetune)\n",
    "    #print(mean_val)\n",
    "    wandb.init(project=f'{architecture} - deep_van_gogh - aug - HP Tuning - 27.3.2025',\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": 128,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{architecture}_trial_{trial.number + 1}\")\n",
    "    wandb.log({'Trial': trial.number+1, 'Mean AUC': mean_val})\n",
    "    wandb.finish()\n",
    "    # mean_val = train_model_with_hyperparams(model, train_loader, learning_rate, weight_decay, criterion,\n",
    "    #                                              epochs=epochs, patience=patience, device=device, trial=trial,\n",
    "    #                                              architecture=architecture)\n",
    "\n",
    "\n",
    "\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return mean_val\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-Validation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def cross_validation_temp(dataset:Dataset, **models_dict):\n",
    "    # Initialize KFold\n",
    "    k_folds = 5\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Track performance for each model\n",
    "    results = defaultdict(list)\n",
    "\n",
    "    for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "            #print(f\"\\tFold {fold + 1}\")\n",
    "            # Subset the dataset for this fold\n",
    "            train_subset = Subset(dataset, train_ids)\n",
    "            val_subset = Subset(dataset, val_ids)\n",
    "\n",
    "            # Create data loaders\n",
    "            train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "            val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "            for model_name, model_dict in models_dict.items():\n",
    "                #print(f\"Training :{model_name}\")\n",
    "                # Load the entire model\n",
    "                base_model = vgg19 if model_dict[\"architecture\"] == VGG19 else alexnet\n",
    "                model = FinedTunedModel(base_model.to(device), model_dict[\"architecture\"]).to(device)\n",
    "\n",
    "                # Define loss function and optimizer\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = optim.Adam(model.parameters(), **model_dict['param_groups'])\n",
    "\n",
    "\n",
    "\n",
    "                # Train the model (implement your training loop here)\n",
    "                best_val = train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion,\n",
    "                                                             epochs=3, patience=3, device=device, trial=None,\n",
    "                                                             architecture=None)\n",
    "                 # Append the results for this fold\n",
    "                results[model_name].append(best_val)\n",
    "\n",
    "\n",
    "    # Print final results\n",
    "    mean_perf_dict = {}\n",
    "    for model_name, model_results in results.items():\n",
    "         # After all folds, calculate the average fold performance\n",
    "        mean_perf = sum(results[model_name]) / len(results[model_name])\n",
    "        print(f\"Average Performance for {model_name}: {mean_perf}\")\n",
    "\n",
    "        print(f\"{model_name} - Cross-Validation Results: {model_results}\")\n",
    "        print(f\"{model_name} - Mean Performance: {sum(model_results) / len(model_results)}\")\n",
    "        mean_perf_dict[model_name] = mean_perf\n",
    "\n",
    "    return mean_perf_dict"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "vgg_path = os.path.join(utils.MODELS_DIR, VGG19)\n",
    "\n",
    "def get_hyperparameters(path):\n",
    "    param_groups = torch.load(path, weights_only=True)['optimizer_state_dict']['param_groups'][0]\n",
    "    return {'lr':param_groups['lr'], 'weight_decay':param_groups['weight_decay']}\n",
    "\n",
    "\n",
    "model1_param_groups = get_hyperparameters(f\"{vgg_path}/best_model_trial_0.pt\") # Load hyperparameters\n",
    "model1_dict = {\n",
    "    'architecture': VGG19,\n",
    "    'param_groups': model1_param_groups\n",
    "}\n",
    "\n",
    "\n",
    "model2_param_groups =  get_hyperparameters(f\"{vgg_path}/best_model_trial_1.pt\") # Load hyperparameters\n",
    "model2_dict = {\n",
    "    'architecture': VGG19,\n",
    "    'param_groups': model2_param_groups\n",
    "}\n",
    "\n",
    "\n",
    "cross_validation(train_dataset, vgg_model1=model1_dict, vgg_model2=model2_dict)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "source": [
    "# Fine tuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gaSN21pPiGdy",
    "outputId": "8101799f-ef2c-4ada-8a3d-7dc594b2385e"
   },
   "source": [
    "# Load the AlexNet model \n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)\n",
    "alexnet_model = FinedTunedModel(alexnet, ALEXNET).to(device)\n",
    "alexnet_model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Optuna for our AlexNet model with the default config\n",
    "study = optuna.create_study(study_name=ALEXNET, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, alexnet_model, config={}), n_trials=15)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:42.705391Z",
     "start_time": "2025-03-30T15:47:42.423671Z"
    }
   },
   "source": [
    "import tqdm\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "#define a function to load an image and pre-process it\n",
    "def load_image(img_path, shape=(224,224)):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    # Define transformation to resize, normalize, and convert to tensor\n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(shape),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    ])\n",
    "    # Apply transformations, remove alpha channel, and add batch dimension\n",
    "    image = in_transform(image)[:3, :, :].unsqueeze(0)\n",
    "    return image.to(device)\n",
    "#define a function to extract features from the network\n",
    "def get_features(image, model, layers):\n",
    "    features = {}\n",
    "    x = image\n",
    "    for name, layer in model._modules.items():\n",
    "        x = layer(x)\n",
    "        if name in layers:\n",
    "            features[layers[name]] = x\n",
    "    return features\n",
    "def gram_matrix(tensor):\n",
    "    _, d, h, w = tensor.size()\n",
    "    tensor = tensor.view(d, h * w)\n",
    "    gram = torch.mm(tensor, tensor.t())\n",
    "    return gram\n",
    "\n",
    "# Deprocess the image by reversing the normalization\n",
    "def deprocess(tensor):\n",
    "    tensor = tensor.cpu().clone().detach()\n",
    "    tensor = tensor.squeeze(0)\n",
    "    tensor = tensor * torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1) + torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    tensor = torch.clamp(tensor, 0, 1)\n",
    "    return to_pil_image(tensor)\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:43.623820Z",
     "start_time": "2025-03-30T15:47:43.605786Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def style_transfer(model, style_img_path, content_img_path, content_weight=1, style_weight=1e3, num_steps=5001, model_name='vgg19_pretrained'):\n",
    "    model = model.features #Gives us access to the layers of features\n",
    "\n",
    "    layers = {\n",
    "         '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "         '21': 'conv4_2'\n",
    "    }\n",
    "\n",
    "    style_weights = {\n",
    "        'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3\n",
    "    }\n",
    "\n",
    "    content_layer = 'conv4_2'\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "         param.requires_grad_(False)\n",
    "    # Load and preprocess the content and style images\n",
    "    content = load_image(content_img_path).to(device)\n",
    "    style = load_image(style_img_path).to(device)\n",
    "    # Extract features from content and style images\n",
    "    content_features = get_features(content, model, layers)\n",
    "    style_features = get_features(style, model, layers)\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    for ii in tqdm.tqdm(range(1, num_steps + 1), desc=\"Style Transfer Progress\"):\n",
    "        # Extract features from target image\n",
    "        target_features = get_features(target, model, layers)\n",
    "\n",
    "        # Compute content loss\n",
    "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "        # Compute style loss by comparing Gram matrices for each layer\n",
    "        style_loss = 0\n",
    "        for layer in style_weights:\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        # Calculate total loss and update target image\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track the loss\n",
    "        if ii % 1000 == 0:\n",
    "            losses.append(total_loss.item())\n",
    "            print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "    # Plot the loss\n",
    "    #plt.plot(range(0, len(losses) * 100, 100), losses)\n",
    "    #plt.xlabel('Step')\n",
    "    #plt.ylabel('Total Loss')\n",
    "    #plt.title('Loss during Style Transfer')\n",
    "    #plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "\n",
    "def style_transfer2(model, style_img_path, content_img_path, layers, style_weights, content_layer, content_weight=1, style_weight=1e3, num_steps=5001,shape=(224,224)):\n",
    "    model = model.features  # Gives us access to the layers of features\n",
    "\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "    # Load and preprocess the content and style images\n",
    "    content = load_image(content_img_path,shape).to(device)\n",
    "    style = load_image(style_img_path,shape).to(device)\n",
    "    # Extract features from content and style images\n",
    "    content_features = get_features(content, model, layers)\n",
    "    style_features = get_features(style, model, layers)\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "\n",
    "    style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    for ii in tqdm.tqdm(range(1, num_steps + 1), desc=\"Style Transfer Progress\"):\n",
    "        # Extract features from target image\n",
    "        target_features = get_features(target, model, layers)\n",
    "\n",
    "        # Compute content loss\n",
    "        content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "        # Compute style loss by comparing Gram matrices for each layer\n",
    "        style_loss = 0\n",
    "        for layer in style_weights:\n",
    "            target_feature = target_features[layer]\n",
    "            target_gram = gram_matrix(target_feature)\n",
    "            _, d, h, w = target_feature.shape\n",
    "            style_gram = style_grams[layer]\n",
    "            layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "            style_loss += layer_style_loss / (d * h * w)\n",
    "        # Calculate total loss and update target image\n",
    "        total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        # Track the loss\n",
    "        if ii % 1000 == 0:\n",
    "            losses.append(total_loss.item())\n",
    "            print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def style_transfer_multi_style(model, style_img_paths, style_weights, content_img_path, layers, style_layer_weights, content_layer, content_weight=1, style_weight=1e3, num_steps=5001, shape=(224,224), show_progress=False):\n",
    "    model = model.features  # Gives us access to the layers of features\n",
    "\n",
    "    # Prepare model for evaluation, disabling gradient computation\n",
    "    model.to(device).eval()\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad_(False)\n",
    "\n",
    "    # Load and preprocess the content image\n",
    "    content = load_image(content_img_path, shape).to(device)\n",
    "    content_features = get_features(content, model, layers)\n",
    "\n",
    "    # Load and preprocess the style images\n",
    "    style_features_list = []\n",
    "    for style_img_path in style_img_paths:\n",
    "        style = load_image(style_img_path, shape).to(device)\n",
    "        style_features = get_features(style, model, layers)\n",
    "        style_features_list.append(style_features)\n",
    "\n",
    "    # Combine style features by weighted averaging\n",
    "    combined_style_features = {}\n",
    "    for layer in layers.values():\n",
    "        combined_style_features[layer] = torch.zeros_like(style_features_list[0][layer])\n",
    "        for i, style_features in enumerate(style_features_list):\n",
    "            combined_style_features[layer] += style_weights[i] * style_features[layer]\n",
    "        combined_style_features[layer] /= sum(style_weights)\n",
    "\n",
    "    target = content.clone().requires_grad_(True).to(device)\n",
    "    style_grams = {layer: gram_matrix(combined_style_features[layer]) for layer in combined_style_features}\n",
    "    optimizer = optim.Adam([target], lr=0.003)\n",
    "\n",
    "    # Initialize list to track losses\n",
    "    losses = []\n",
    "    # Style transfer loop\n",
    "    with tqdm.tqdm(total=num_steps, desc=\"Style Transfer Progress\") as pbar:\n",
    "        # Style transfer loop\n",
    "        for ii in range(1, num_steps + 1):\n",
    "            # Extract features from target image\n",
    "            target_features = get_features(target, model, layers)\n",
    "            # Compute content loss\n",
    "            content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n",
    "\n",
    "            # Compute style loss by comparing Gram matrices for each layer\n",
    "            style_loss = 0\n",
    "            for layer in style_layer_weights:\n",
    "                target_feature = target_features[layer]\n",
    "                target_gram = gram_matrix(target_feature)\n",
    "                _, d, h, w = target_feature.shape\n",
    "                style_gram = style_grams[layer]\n",
    "                layer_style_loss = style_layer_weights[layer] * torch.mean((target_gram - style_gram)**2)\n",
    "                style_loss += layer_style_loss / (d * h * w)\n",
    "\n",
    "            # Calculate total loss and update target image\n",
    "            total_loss = content_weight * content_loss + style_weight * style_loss\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            pbar.update(1)\n",
    "\n",
    "            # Track the loss\n",
    "            if ii % 1000 == 0:\n",
    "                losses.append(total_loss.item())\n",
    "                print(f\"Step {ii}, Total loss: {total_loss.item()}\")\n",
    "                if show_progress:\n",
    "                    # Deprocess and display the intermediate result\n",
    "                    intermediate_result = deprocess(target)\n",
    "                    plt.imshow(intermediate_result)\n",
    "                    plt.title(f\"Step {ii}\")\n",
    "                    plt.axis('off')\n",
    "                    plt.show()\n",
    "\n",
    "    return target\n",
    "\n",
    "\n",
    "def style_transfer_multi_wrapper(model, style_img_paths, style_weights_list, content_img_path, layers, style_layer_weights_list, content_layer, content_weight_list, style_weight_list, num_steps=5001, shape=(224,224), show_progress=False):\n",
    "    results = []\n",
    "    for style_weights, style_layer_weights, content_weight, style_weight in zip(style_weights_list, style_layer_weights_list, content_weight_list, style_weight_list):\n",
    "        # Call the multi-style transfer function\n",
    "        target = style_transfer_multi_style(model, style_img_paths, style_weights, content_img_path, layers, style_layer_weights, content_layer, content_weight=content_weight, style_weight=style_weight, num_steps=num_steps, shape=shape, show_progress=show_progress)\n",
    "        results.append(deprocess(target))\n",
    "    return results\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:47:45.452529Z",
     "start_time": "2025-03-30T15:47:45.443206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#saving functions\n",
    "import shutil\n",
    "\n",
    "def generate_folder_name(style_img_paths, content_img_path, model_name):\n",
    "    style_imgs = '_'.join([os.path.basename(path).split('.')[0] for path in style_img_paths])\n",
    "    content_img = os.path.basename(content_img_path).split('.')[0]\n",
    "    folder_name = f\"{model_name}_style_{style_imgs}_content_{content_img}\"\n",
    "    return folder_name\n",
    "\n",
    "def generate_filename(style_weights, style_layer_weights, content_weight, style_weight, layers, content_layer, index):\n",
    "    style_weights_str = '_'.join(map(str, style_weights))\n",
    "    style_layer_weights_str = '_'.join([f\"{k}-{v}\" for k, v in style_layer_weights.items()])\n",
    "    layers_str = '_'.join(layers.keys())\n",
    "    filename = f\"output_{index}_sw_{style_weights_str}_slw_{style_layer_weights_str}_cw_{content_weight}_sw_{style_weight}_layers_{layers_str}_cl_{content_layer}.jpg\"\n",
    "    return filename\n",
    "\n",
    "def create_directory(base_dir, folder_name):\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "    return folder_path\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T15:48:16.087515Z",
     "start_time": "2025-03-30T15:48:13.902304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "def load_model(model_path, architecture):\n",
    "    if architecture == 'vgg19':\n",
    "        model = models.vgg19(pretrained=False)\n",
    "        # Modify the classifier to match the saved model\n",
    "        model.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "    elif architecture == 'alexnet':\n",
    "        model = models.alexnet(pretrained=False)\n",
    "        # Modify the classifier to match the saved model\n",
    "        model.classifier[6] = torch.nn.Linear(4096, 2)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported architecture: {architecture}\")\n",
    "\n",
    "    checkpoint = torch.load(model_path, weights_only=False)\n",
    "    state_dict = checkpoint['model_state_dict']\n",
    "\n",
    "    # Rename keys in state_dict to match the model's expected keys\n",
    "    new_state_dict = {}\n",
    "    for k, v in state_dict.items():\n",
    "        new_key = k.replace('features_extractor.0.', 'features.')\n",
    "        new_state_dict[new_key] = v\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "model_path = r'D:\\Users\\ghefter\\deep_van_gogh\\models\\AlexNet\\AlexNet_best_model.pt'\n",
    "architecture = 'alexnet'  # Corrected architecture to match the model file\n",
    "alexnet_fine_tuned = load_model(model_path, architecture)\n",
    "alexnet_fine_tuned.eval()  # Set the model to evaluation mode\n",
    "model_path = r'D:\\Users\\ghefter\\deep_van_gogh\\models\\VGG19\\VGG19_best_model.pt'\n",
    "architecture = 'vgg19'  # Corrected architecture to match the model file\n",
    "vgg_fine_tuned = load_model(model_path, architecture)\n",
    "vgg_fine_tuned.eval()\n",
    "\n",
    "# Now you can use the loaded model in your style transfer process"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ghefter\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\ghefter\\.conda\\envs\\DeepLearning_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VGG(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (17): ReLU(inplace=True)\n",
       "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (24): ReLU(inplace=True)\n",
       "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (26): ReLU(inplace=True)\n",
       "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (31): ReLU(inplace=True)\n",
       "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (33): ReLU(inplace=True)\n",
       "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (35): ReLU(inplace=True)\n",
       "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Dropout(p=0.5, inplace=False)\n",
       "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): Dropout(p=0.5, inplace=False)\n",
       "    (6): Linear(in_features=4096, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "alexnet_fine_tuned.features",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# Example usage\n",
    "style_img_paths = [r'D:\\Users\\ghefter\\deep_van_gogh\\dataset\\Post_Impressionism\\vincent-van-gogh_wheat-stacks-with-reaper-1888.jpg',r'D:\\Users\\ghefter\\deep_van_gogh\\dataset\\Post_Impressionism\\vincent-van-gogh_two-peasant-women-digging-in-field-with-snow-1890.jpg']\n",
    "content_img_path = r'D:\\Users\\ghefter\\deep_van_gogh\\dataset\\Post_Impressionism\\vincent-van-gogh_flowerpot-with-chives-1887(1).jpg'\n",
    "\n",
    "style_weights_list = [\n",
    "    [0.5, 0.5],\n",
    "    [0.2, 0.8],\n",
    "    [0.8, 0.2],\n",
    "    [0.3, 0.7],\n",
    "    [0.7, 0.3]\n",
    "]\n",
    "\n",
    "style_layer_weights_list = [\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3}\n",
    "]\n",
    "\n",
    "content_weight_list = [1, 1, 1, 1, 1]\n",
    "style_weight_list = [1e3,1e3,1e3,1e3,1e3]\n",
    "\n",
    "model = vgg_fine_tuned\n",
    "layers = {\n",
    "    '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "    '21': 'conv4_2'\n",
    "}\n",
    "content_layer = 'conv4_2'\n",
    "\n",
    "results = style_transfer_multi_wrapper(model, style_img_paths, style_weights_list, content_img_path, layers, style_layer_weights_list, content_layer, content_weight_list, style_weight_list, num_steps=5001, shape=(224,224), show_progress=False)\n",
    "\n",
    "#save the results\n",
    "output_dir = r'D:\\Users\\ghefter\\deep_van_gogh\\data\\stylized_images'\n",
    "model_name = 'vgg_fine_tuned'  # or 'alexnet_fine_tuned'\n",
    "\n",
    "for i, (img, style_weights, style_layer_weights, content_weight, style_weight) in enumerate(zip(results, style_weights_list, style_layer_weights_list, content_weight_list, style_weight_list)):\n",
    "    folder_name = generate_folder_name(style_img_paths, content_img_path, model_name)\n",
    "    folder_path = create_directory(output_dir, folder_name)\n",
    "    filename = generate_filename(style_weights, style_layer_weights, content_weight, style_weight, layers, content_layer, i)\n",
    "    output_path = os.path.join(folder_path, filename)\n",
    "    img.save(output_path)\n",
    "\n",
    "    # Copy original style and content images to the directory\n",
    "    for style_img_path in style_img_paths:\n",
    "        shutil.copy(style_img_path, folder_path)\n",
    "    shutil.copy(content_img_path, folder_path)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2UNqmGA2DXDj",
    "outputId": "18e5c745-d27d-4f23-97cf-717b194bd3c3",
    "ExecuteTime": {
     "end_time": "2025-03-30T15:34:02.671805Z",
     "start_time": "2025-03-30T15:30:48.159800Z"
    }
   },
   "source": [
    "#Idan's images\n",
    "style_img_paths = [r\"D:\\Users\\ghefter\\deep_van_gogh\\data\\Idan's photos\\vincent-van-gogh_the-starry-night-1889(1).jpg\"]\n",
    "content_img_path = r\"D:\\Users\\ghefter\\deep_van_gogh\\data\\Idan's photos\\Eze Edited style transfer 4.7.2024.jpeg\"\n",
    "\n",
    "style_weights_list = [\n",
    "    [1],\n",
    "    #[1],\n",
    "    #[1],\n",
    "    #[1]\n",
    "]\n",
    "\n",
    "style_layer_weights_list = [\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3}\n",
    "]\n",
    "\n",
    "content_weight_list = [1,\n",
    "                       # 1,1,1\n",
    "                        ]\n",
    "style_weight_list = [1e3,\n",
    "                     #1e4,1e5,1e6\n",
    "                      ]\n",
    "\n",
    "model = vgg19\n",
    "layers = {\n",
    "    '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "    '21': 'conv4_2'\n",
    "}\n",
    "content_layer = 'conv4_2'\n",
    "\n",
    "results = style_transfer_multi_wrapper(model, style_img_paths, style_weights_list, content_img_path, layers, style_layer_weights_list, content_layer, content_weight_list, style_weight_list, num_steps=5001, shape=(1024,461), show_progress=False)\n",
    "\n",
    "#save the results\n",
    "output_dir = r'D:\\Users\\ghefter\\deep_van_gogh\\data\\stylized_images'\n",
    "model_name = 'vgg_pretrained'  # or 'alexnet_fine_tuned'\n",
    "\n",
    "for i, (img, style_weights, style_layer_weights, content_weight, style_weight) in enumerate(zip(results, style_weights_list, style_layer_weights_list, content_weight_list, style_weight_list)):\n",
    "    folder_name = generate_folder_name(style_img_paths, content_img_path, model_name)\n",
    "    folder_path = create_directory(output_dir, folder_name)\n",
    "    filename = generate_filename(style_weights, style_layer_weights, content_weight, style_weight, layers, content_layer, i)\n",
    "    output_path = os.path.join(folder_path, filename)\n",
    "    img.save(output_path)\n",
    "\n",
    "    # Copy original style and content images to the directory\n",
    "    for style_img_path in style_img_paths:\n",
    "        shutil.copy(style_img_path, folder_path)\n",
    "    shutil.copy(content_img_path, folder_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  20%|        | 1008/5001 [00:38<02:21, 28.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Total loss: 8116.3505859375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  40%|      | 2008/5001 [01:17<01:50, 27.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Total loss: 2601.2021484375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  60%|    | 3008/5001 [01:56<01:14, 26.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, Total loss: 1264.151611328125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  80%|  | 4008/5001 [02:35<00:36, 27.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4000, Total loss: 732.0604248046875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress: 100%|| 5001/5001 [03:14<00:00, 25.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5000, Total loss: 426.65399169921875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dC9MS7V6WZF",
    "outputId": "80757501-0728-4fe1-96b7-a15122faad92",
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-30T15:55:19.007328Z"
    }
   },
   "source": [
    "#Amit's images\n",
    "style_img_paths = [r\"D:\\Users\\ghefter\\deep_van_gogh\\data\\Amit's photos\\vincent-van-gogh_self-portrait-1887-9.jpg\"]\n",
    "content_img_path = r\"D:\\Users\\ghefter\\deep_van_gogh\\data\\Amit's photos\\IMG_9671.jpeg\"\n",
    "\n",
    "style_weights_list = [\n",
    "    [1],\n",
    "    [1],\n",
    "    #[1],\n",
    "    #[1]\n",
    "]\n",
    "\n",
    "style_layer_weights_list = [\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    {'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3},\n",
    "    #{'conv1_1': 0.5, 'conv2_1': 0.5, 'conv3_1': 0.5, 'conv4_1': 0.3}\n",
    "]\n",
    "\n",
    "content_weight_list = [1,\n",
    "                       # 1,1,\n",
    "                        1\n",
    "                        ]\n",
    "style_weight_list = [1e3,\n",
    "                     #1e4,1e5,\n",
    "                      1e6\n",
    "                      ]\n",
    "\n",
    "model = vgg19\n",
    "layers = {\n",
    "    '0': 'conv1_1', '5': 'conv2_1', '10': 'conv3_1', '19': 'conv4_1',\n",
    "    '21': 'conv4_2'\n",
    "}\n",
    "content_layer = 'conv4_2'\n",
    "\n",
    "results = style_transfer_multi_wrapper(model, style_img_paths, style_weights_list, content_img_path, layers, style_layer_weights_list, content_layer, content_weight_list, style_weight_list, num_steps=5001, shape=(806,604), show_progress=False)\n",
    "\n",
    "#save the results\n",
    "output_dir = r'D:\\Users\\ghefter\\deep_van_gogh\\data\\stylized_images'\n",
    "model_name = 'vgg_pretrained'  # or 'alexnet_fine_tuned'\n",
    "\n",
    "for i, (img, style_weights, style_layer_weights, content_weight, style_weight) in enumerate(zip(results, style_weights_list, style_layer_weights_list, content_weight_list, style_weight_list)):\n",
    "    folder_name = generate_folder_name(style_img_paths, content_img_path, model_name)\n",
    "    folder_path = create_directory(output_dir, folder_name)\n",
    "    filename = generate_filename(style_weights, style_layer_weights, content_weight, style_weight, layers, content_layer, i)\n",
    "    output_path = os.path.join(folder_path, filename)\n",
    "    img.save(output_path)\n",
    "\n",
    "    # Copy original style and content images to the directory\n",
    "    for style_img_path in style_img_paths:\n",
    "        shutil.copy(style_img_path, folder_path)\n",
    "    shutil.copy(content_img_path, folder_path)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  20%|        | 1007/5001 [00:40<02:27, 27.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000, Total loss: 8571.2724609375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  40%|      | 2008/5001 [01:20<01:52, 26.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 2000, Total loss: 3438.705810546875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  60%|    | 3007/5001 [02:00<01:14, 26.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3000, Total loss: 1906.2593994140625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Style Transfer Progress:  79%|  | 3965/5001 [02:38<00:40, 25.37it/s]"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w93PNhRgHVue",
    "outputId": "704e1556-8d2f-4c90-af0e-bc26aa7a072e"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "24UKrxg-H25c"
   },
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
