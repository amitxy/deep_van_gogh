{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:34:48.687114Z",
     "start_time": "2025-04-01T07:34:37.577014Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sympy.physics.units import length\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import vgg\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from collections import defaultdict\n",
    "from optuna import trial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import optuna\n",
    "import wandb\n",
    "# Project utilities\n",
    "import utils\n",
    "import importlib\n",
    "import train\n",
    "importlib.reload(train)\n",
    "importlib.reload(utils)\n",
    "from train import train_model_with_hyperparams\n",
    "\n",
    "VGG19 = 'VGG19'\n",
    "ALEXNET = 'AlexNet'\n",
    "\n",
    "# Set seed\n",
    "SEED = utils.SEED\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:34:48.750087Z",
     "start_time": "2025-04-01T07:34:48.692088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check if you're working locally or not\n",
    "if not (os.path.exists(utils.CSV_PATH) and os.path.exists(utils.OPTIMIZED_DIR)):\n",
    "    print(f\"[!] You are NOT on the project's directory [!]\\n\"\n",
    "          f\"Please run the following command (in either CMD or anaconda prompt): \\n\"\n",
    "          f\"jupyter notebook --notebook-dir PROJECT_DIR\\n\"\n",
    "          r\"Where PROJECT_DIR is the project's directory in your computer e.g: C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading our data\n",
    "We will load the optimized datasets from our custom dataset object\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:34:59.592732Z",
     "start_time": "2025-04-01T07:34:48.845699Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumPyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.images = data[\"images\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        idx = idx % len(self.images)\n",
    "        x = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, 'dataset.npz'))"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can find the optimized dataset files <a href=\"https://drive.google.com/drive/folders/16vIyBwzvGgC-bJObJZT-RgZJmh3fj4Vt?usp=drive_link\">HERE inside the data folder</a>. Note that you must have the data folder in the project directory<br/>\n",
    "Loading the train and test datasets:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:34:59.685561Z",
     "start_time": "2025-04-01T07:34:59.610734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = pd.read_csv(utils.CSV_PATH)\n",
    "train_rows = classes[classes['subset'] == 'train']\n",
    "\n",
    "# train_indices, val_indices = train_test_split(train_rows.index.to_list(), test_size=0.2, random_state=utils.SEED, stratify=train_rows['is_van_gogh'])\n",
    "# train_dataset = Subset(dataset, train_indices)\n",
    "# val_dataset = Subset(dataset, val_indices)\n",
    "train_dataset = Subset(dataset, train_rows.index.tolist())\n",
    "test_dataset = Subset(dataset, classes[classes['subset'] == 'test'].index.tolist())"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:13.251746Z",
     "start_time": "2025-04-01T07:34:59.719901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_opt_dataset(dataset_name):\n",
    "    return NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, f'{dataset_name}.npz'))\n",
    "\n",
    "flip_dataset = get_opt_dataset('flip')\n",
    "dropout_dataset = get_opt_dataset('dropout')\n",
    "affine_dataset = get_opt_dataset('affine')\n",
    "blur_dataset = get_opt_dataset('blur')\n",
    "augmented_train_dataset = ConcatDataset([train_dataset, flip_dataset, dropout_dataset, affine_dataset, blur_dataset])"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a detailed explanation about our data augmentation, please check augmentation_demo.ipynb"
  },
  {
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "cell_type": "markdown",
   "source": "# Fine tuning VGG19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:13.297975Z",
     "start_time": "2025-04-01T07:35:13.284975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FinedTunedModel(nn.Module):\n",
    "    def __init__(self, base_model, architecture:str):\n",
    "        super(FinedTunedModel, self).__init__()\n",
    "        self._architecture = architecture  # Save the base model architecture\n",
    "        base_children_list = list(base_model.children())\n",
    "        self.features_extractor = nn.Sequential(*base_children_list[:-1]).to(device)\n",
    "        for param in self.features_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to fit to our problem (2 classes)\n",
    "        self.classifier = nn.Sequential(*base_children_list[-1])\n",
    "        self.classifier[-1] = nn.Linear(4096, 2).to(device)  # Replaces the final layer of the base model's classifier with a new fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_model_output = self.features_extractor(x)\n",
    "        return self.classifier(torch.flatten(base_model_output, start_dim=1))\n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self._architecture"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:20.454793Z",
     "start_time": "2025-04-01T07:35:13.317975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained models\n",
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device)\n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:20.514793Z",
     "start_time": "2025-04-01T07:35:20.488795Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation(learning_rate,\n",
    "                     weight_decay,\n",
    "                     num_layers_finetune,\n",
    "                     criterion,\n",
    "                     epochs,\n",
    "                     patience,\n",
    "                     device,\n",
    "                     architecture,\n",
    "                     batch_size=128, trial=None, project='project'):\n",
    "    k_folds = 4\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create a label list for the augmented dataset\n",
    "    labels = [train_rows['is_van_gogh']]\n",
    "    for i in range(1, len(augmented_train_dataset.datasets)):\n",
    "        ds = augmented_train_dataset.datasets[i]\n",
    "        labels.append(ds.labels)\n",
    "    labels = np.concatenate(labels)\n",
    "\n",
    "\n",
    "    # Track performance for each model\n",
    "    base_model = vgg19 if architecture == VGG19 else alexnet\n",
    "    metrics_list = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(labels)), labels)):\n",
    "        wandb.init(project = project,\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"fold\": fold + 1,\n",
    "                                \"trial\": trial.number + 1,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f'{architecture}_trial_{trial.number + 1 if trial else -1}_fold_{fold+1}')\n",
    "\n",
    "        train_subset = Subset(augmented_train_dataset, train_idx)\n",
    "        val_subset = Subset(augmented_train_dataset, val_idx)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = FinedTunedModel(base_model, architecture).to(device)\n",
    "        if num_layers_finetune:\n",
    "            for param in model.features_extractor[-num_layers_finetune:].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Train the model\n",
    "        mean_metrics = train_model_with_hyperparams(model,\n",
    "                                                train_loader,\n",
    "                                                val_loader,\n",
    "                                                optimizer,\n",
    "                                                criterion,\n",
    "                                                epochs=epochs,\n",
    "                                                patience=patience,\n",
    "                                                device=device,\n",
    "                                                trial=trial,\n",
    "                                                architecture=architecture, fold=fold + 1)\n",
    "        metrics_list.append(mean_metrics)\n",
    "        # Finish the Weights & Biases run\n",
    "        wandb.finish()\n",
    "    mean_dict = utils.mean_dict(metrics_list)\n",
    "    return mean_dict\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:20.576258Z",
     "start_time": "2025-04-01T07:35:20.548792Z"
    }
   },
   "source": [
    "def objective(trial, architecture, config: dict) -> float:\n",
    "    \"\"\"\n",
    "    Generic Optuna objective function.\n",
    "    :param trial: Optuna trial object.\n",
    "    :param model: The neural network model to train\n",
    "    :param config: A dictionary with configurable values such as learning rate ranges, batch size ranges, etc.\n",
    "    :return:  best_val_loss: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions based on config\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\",\n",
    "                                        config.get(\"lr_min\", 1e-5),\n",
    "                                        config.get(\"lr_max\", 1e-3),\n",
    "                                        log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\",\n",
    "                                       config.get(\"wd_min\", 1e-6),\n",
    "                                       config.get(\"wd_max\", 1e-4),\n",
    "                                       log=True)\n",
    "    # Including the option not to perform fine-tuning, or only a small num of layers within the feature extractor.\n",
    "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 4)\n",
    "    batch_min = config.get(\"batch_size_min\", 32)\n",
    "    batch_max =config.get(\"batch_size_max\", 256)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",\n",
    "                                   [2**i for i in range(int(np.log2(batch_min)), int(np.log2(batch_max))+1)]\n",
    "                                   )\n",
    "\n",
    "    epochs = trial.suggest_int(\"epochs\", config.get(\"epochs_min\", 10), config.get(\"epochs_max\",30))\n",
    "    # patience = trial.suggest_int(\"patience\", config.get(\"patience_min\", 5), config.get(\"patience_max\", 15))\n",
    "    patience = 5\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = config.get(\"criterion\", nn.CrossEntropyLoss()) # Classification.\n",
    "\n",
    "    project = config.get(\"project\", 'deep_van_gogh_default')\n",
    "    # Train the model and get the best mean val_auc\n",
    "    mean_dict = cross_validation(learning_rate, weight_decay, num_layers_finetune, criterion, epochs, patience, device, architecture, batch_size, trial, project=project)\n",
    "    # Log the mean values\n",
    "\n",
    "    wandb.init(project = project,\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"trial\": trial.number + 1,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{architecture}_trial_{trial.number + 1 if trial else -1}\")\n",
    "\n",
    "    wandb.log(mean_dict)\n",
    "    wandb.finish()\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return mean_dict['Validation AUC']\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-Validation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "n_trials = 15\n",
    "study = optuna.create_study(study_name=VGG19, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, VGG19, config={'project': 'VGG-19-CV_FINAL_Run'}), n_trials=n_trials)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:20.623260Z",
     "start_time": "2025-04-01T07:35:20.610262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_hyperparameters_from_best_model(architecture, trial_num, fold_num):\n",
    "    hyperparameters = torch.load(f\"models/{architecture}/{architecture}_best_model_trial_{trial_num}_fold_{fold_num}.pt\",\n",
    "                            weights_only=False)['hyperparameters']\n",
    "    return hyperparameters\n",
    "\n",
    "print('Best trial:')\n",
    "# hyperparameters = get_hyperparameters_from_best_model(VGG, 2, 1)\n",
    "# hyperparameters"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T05:37:25.546753Z",
     "start_time": "2025-04-01T05:37:25.441479Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 97,
   "source": [
    "train_indices, val_indices = train_test_split(train_rows.index.to_list(), test_size=0.2, random_state=utils.SEED, stratify=train_rows['is_van_gogh'])\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T07:35:20.685614Z",
     "start_time": "2025-04-01T07:35:20.658259Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_model_from_cv(architecture,\n",
    "                        hyperparameters,\n",
    "                        project_name = 'Final_Models_Run',\n",
    "                        log = True,\n",
    "                        save_model = True):\n",
    "    base_model = alexnet if architecture == ALEXNET else vgg\n",
    "    model = FinedTunedModel(base_model.to(device), architecture).to(device)\n",
    "    weight_decay = hyperparameters['weight_decay']\n",
    "    lr = hyperparameters['learning_rate']\n",
    "    batch_size = hyperparameters['batch_size']\n",
    "    epochs = hyperparameters['epochs']\n",
    "    patience = hyperparameters['patience']\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    wandb.init(project=project_name,\n",
    "               config={ \"learning_rate\": lr,\n",
    "                        \"weight_decay\": weight_decay,\n",
    "                        \"patience\": patience,\n",
    "                        \"batch_size\": batch_size,\n",
    "                        \"epochs\": epochs,\n",
    "                        \"architecture\": architecture,\n",
    "                        \"dataset\": \"Post_Impressionism\",\n",
    "                        }, name=f\"{architecture}-best\")\n",
    "\n",
    "    train_model_with_hyperparams(model, train_loader, val_loader, optimizer, criterion, epochs=epochs, patience=patience,device=device, trial=None, architecture=architecture, fold=-1, save_model=True, log=True)\n",
    "    mean_values = train.validation(model, criterion, test_loader, device, is_test=True)\n",
    "    wandb.log(mean_values)\n",
    "    wandb.finish()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "# train_model_from_cv(VGG19, hyperparameters,project_name = 'Final_Models_RUN')"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "cell_type": "markdown",
   "source": "# Fine tuning AlexNet"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_trials = 15\n",
    "study = optuna.create_study(study_name=ALEXNET, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, ALEXNET, config={'project':'AlexNet-CV_FINAL_Run'}), n_trials=n_trials)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model - ALEXNET"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print('Best trial:')\n",
    "# hyperparameters = get_hyperparameters_from_best_model(VGG, 2, 1)\n",
    "# hyperparameters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# train_model_from_cv(ALEXNET, hyperparameters,project_name = 'Final_Models_RUN')"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-01T05:38:46.398039Z",
     "start_time": "2025-04-01T05:38:45.331489Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[102], line 6\u001B[0m\n\u001B[0;32m      4\u001B[0m weight_decay \u001B[38;5;241m=\u001B[39m hyperparameters[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweight_decay\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      5\u001B[0m lr \u001B[38;5;241m=\u001B[39m hyperparameters[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlearning_rate\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[1;32m----> 6\u001B[0m batch_size \u001B[38;5;241m=\u001B[39m \u001B[43mhyperparameters\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mbatch_size\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\n\u001B[0;32m      7\u001B[0m epochs \u001B[38;5;241m=\u001B[39m hyperparameters[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mepochs\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      8\u001B[0m patience \u001B[38;5;241m=\u001B[39m hyperparameters[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpatience\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "\u001B[1;31mKeyError\u001B[0m: 'batch_size'"
     ]
    }
   ],
   "execution_count": 102,
   "source": [
    "# Training the model - ALEXNET\n",
    "# base_model = alexnet\n",
    "# alexnet_model = FinedTunedModel(base_model.to(device), ALEXNET).to(device)\n",
    "# weight_decay = hyperparameters['weight_decay']\n",
    "# lr = hyperparameters['learning_rate']\n",
    "# batch_size = hyperparameters['batch_size']\n",
    "# epochs = hyperparameters['epochs']\n",
    "# patience = hyperparameters['patience']\n",
    "# optimizer = optim.Adam(alexnet_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "# train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "#\n",
    "# wandb.init(project='Final_Models_Run',\n",
    "#                        config={ \"learning_rate\": lr,\n",
    "#                                 \"weight_decay\": weight_decay,\n",
    "#                                 \"patience\": patience,\n",
    "#                                 \"batch_size\": batch_size,\n",
    "#                                 \"epochs\": epochs,\n",
    "#                                 \"architecture\": ALEXNET,\n",
    "#                                 \"dataset\": \"Post_Impressionism\",\n",
    "#                                 }, name=f\"{ALEXNET}-best\")\n",
    "#\n",
    "# train_model_with_hyperparams(alexnet_model, train_loader, val_loader, optimizer, criterion, epochs=epochs, patience=patience,device=device, trial=None, architecture=ALEXNET, fold=-1, save_model=True, log=True)\n",
    "#\n",
    "# best_values = train.validation(alexnet_model, criterion, test_loader, device, is_test=True)\n",
    "# wandb.log(best_values)\n",
    "# wandb.finish()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
