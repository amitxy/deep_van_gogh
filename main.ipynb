{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:10:37.179772Z",
     "start_time": "2025-03-31T06:10:37.120353Z"
    }
   },
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sympy.physics.units import length\n",
    "from torchvision import transforms, models\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from collections import defaultdict\n",
    "from optuna import trial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
    "import optuna\n",
    "import wandb\n",
    "# Project utilities\n",
    "import utils\n",
    "import importlib\n",
    "import train\n",
    "importlib.reload(train)\n",
    "importlib.reload(utils)\n",
    "from train import train_model_with_hyperparams\n",
    "\n",
    "VGG19 = 'VGG19'\n",
    "ALEXNET = 'AlexNet'\n",
    "\n",
    "# Set seed\n",
    "SEED = utils.SEED\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n"
   ],
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:10:37.304389Z",
     "start_time": "2025-03-31T06:10:37.291658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Check if you're working locally or not\n",
    "if not (os.path.exists(utils.CSV_PATH) and os.path.exists(utils.OPTIMIZED_DIR)):\n",
    "    print(f\"[!] You are NOT on the project's directory [!]\\n\"\n",
    "          f\"Please run the following command (in either CMD or anaconda prompt): \\n\"\n",
    "          f\"jupyter notebook --notebook-dir PROJECT_DIR\\n\"\n",
    "          r\"Where PROJECT_DIR is the project's directory in your computer e.g: C:\\Users\\amitr5\\PycharmProjects\\deep_van_gogh\")"
   ],
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Loading our data\n",
    "We will load the optimized datasets from our custom dataset object\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:10:48.808649Z",
     "start_time": "2025-03-31T06:10:37.593958Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NumPyDataset(Dataset):\n",
    "    def __init__(self, file_path):\n",
    "        data = np.load(file_path)\n",
    "        self.images = data[\"images\"]\n",
    "        self.labels = data[\"labels\"]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.images[idx], dtype=torch.float32)\n",
    "        y = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "dataset = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, 'dataset.npz'))"
   ],
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "You can find the optimized dataset files <a href=\"https://drive.google.com/drive/folders/16vIyBwzvGgC-bJObJZT-RgZJmh3fj4Vt?usp=drive_link\">HERE inside the data folder</a>. Note that you must have the data folder in the project directory<br/>\n",
    "Loading the train and test datasets:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:10:49.672599Z",
     "start_time": "2025-03-31T06:10:48.887938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "classes = pd.read_csv(utils.CSV_PATH)\n",
    "train_rows = classes[classes['subset'] == 'train']\n",
    "train_indices, val_indices = train_test_split(train_rows.index.to_list(), test_size=0.2, random_state=utils.SEED, stratify=train_rows['is_van_gogh'])\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "test_dataset = Subset(dataset, classes[classes['subset'] == 'test'].index.tolist()).dataset"
   ],
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:08.045198Z",
     "start_time": "2025-03-31T06:10:50.001291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_opt_dataset(dataset_name, train_idx=None, val_idx=None):\n",
    "    data = NumPyDataset(os.path.join(utils.OPTIMIZED_DIR, f'{dataset_name}.npz'))\n",
    "    if train_idx and val_idx:\n",
    "        return Subset(data, train_idx), Subset(data, val_idx)\n",
    "\n",
    "    return Subset(data, train_idx if train_idx else val_idx).dataset\n",
    "\n",
    "flip_dataset = get_opt_dataset('flip', train_indices)\n",
    "dropout_dataset = get_opt_dataset('dropout',train_indices)\n",
    "affine_dataset = get_opt_dataset('affine', train_indices)\n",
    "blur_dataset = get_opt_dataset('blur', train_indices)\n",
    "augmented_train_dataset = ConcatDataset([train_dataset, flip_dataset, dropout_dataset, affine_dataset, blur_dataset])"
   ],
   "outputs": [],
   "execution_count": 77
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Augmentation"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "For a detailed explanation about our data augmentation, please check augmentation_demo.ipynb"
  },
  {
   "metadata": {
    "id": "jFD4XwIchub9"
   },
   "cell_type": "markdown",
   "source": "# Fine tuning VGG19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:08.106989Z",
     "start_time": "2025-03-31T06:11:08.078968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FinedTunedModel(nn.Module):\n",
    "    def __init__(self, base_model, architecture:str):\n",
    "        super(FinedTunedModel, self).__init__()\n",
    "        self._architecture = architecture  # Save the base model architecture\n",
    "        base_children_list = list(base_model.children())\n",
    "        self.features_extractor = nn.Sequential(*base_children_list[:-1]).to(device)\n",
    "        for param in self.features_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the classifier to fit to our problem (2 classes)\n",
    "        self.classifier = nn.Sequential(*base_children_list[-1])\n",
    "        self.classifier[-1] = nn.Linear(4096, 2).to(device)  # Replaces the final layer of the base model's classifier with a new fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_model_output = self.features_extractor(x)\n",
    "        return self.classifier(torch.flatten(base_model_output, start_dim=1))\n",
    "    @property\n",
    "    def architecture(self):\n",
    "        return self._architecture"
   ],
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:14.911777Z",
     "start_time": "2025-03-31T06:11:08.141358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load pre-trained models\n",
    "vgg19 = models.vgg19(weights=models.VGG19_Weights.DEFAULT).to(device)\n",
    "alexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT).to(device)"
   ],
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:15.753670Z",
     "start_time": "2025-03-31T06:11:15.633308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def cross_validation(learning_rate,\n",
    "                     weight_decay,\n",
    "                     num_layers_finetune,\n",
    "                     criterion,\n",
    "                     epochs,\n",
    "                     patience,\n",
    "                     device,\n",
    "                     architecture,\n",
    "                     batch_size=128, trial=None, project='project'):\n",
    "    k_folds = 4\n",
    "    kfold = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    labels = []\n",
    "    for dataset in augmented_train_dataset.datasets:\n",
    "        labels.extend([label for _, label in dataset])\n",
    "\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Track performance for each model\n",
    "    base_model = vgg19 if architecture == VGG19 else alexnet\n",
    "    best_values = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(np.zeros(len(labels)), labels)):\n",
    "        wandb.init(project = project,\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"fold\": fold + 1,\n",
    "                                \"trial\": trial.number + 1,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f'{architecture}_trial_{trial.number + 1 if trial else -1}_fold_{fold+1}')\n",
    "\n",
    "        train_subset = Subset(augmented_train_dataset, train_idx)\n",
    "        val_subset = Subset(augmented_train_dataset, val_idx)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "        model = FinedTunedModel(base_model, architecture).to(device)\n",
    "        if num_layers_finetune:\n",
    "            for param in model.features_extractor[-num_layers_finetune:].parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        # Train the model\n",
    "        best_metrics = train_model_with_hyperparams(model,\n",
    "                                                train_loader,\n",
    "                                                val_loader,\n",
    "                                                optimizer,\n",
    "                                                criterion,\n",
    "                                                epochs=epochs,\n",
    "                                                patience=patience,\n",
    "                                                device=device,\n",
    "                                                trial=trial,\n",
    "                                                architecture=architecture, fold=fold + 1)\n",
    "        best_values.append(best_metrics)\n",
    "        # Finish the Weights & Biases run\n",
    "        wandb.finish()\n",
    "    mean_dict = utils.mean_dict(best_values)\n",
    "    return mean_dict\n"
   ],
   "outputs": [],
   "execution_count": 80
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:15.933927Z",
     "start_time": "2025-03-31T06:11:15.907132Z"
    }
   },
   "source": [
    "def objective(trial, architecture, config: dict) -> float:\n",
    "    \"\"\"\n",
    "    Generic Optuna objective function.\n",
    "    :param trial: Optuna trial object.\n",
    "    :param model: The neural network model to train\n",
    "    :param config: A dictionary with configurable values such as learning rate ranges, batch size ranges, etc.\n",
    "    :return:  best_val_loss: The best validation loss achieved during training.\n",
    "    \"\"\"\n",
    "    # Hyperparameter suggestions based on config\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\",\n",
    "                                        config.get(\"lr_min\", 1e-5),\n",
    "                                        config.get(\"lr_max\", 1e-3),\n",
    "                                        log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\",\n",
    "                                       config.get(\"wd_min\", 1e-6),\n",
    "                                       config.get(\"wd_max\", 1e-4),\n",
    "                                       log=True)\n",
    "    # Including the option not to perform fine-tuning, or only a small num of layers within the feature extractor.\n",
    "    num_layers_finetune = trial.suggest_int(\"num_layers_finetune\", 0, 3)\n",
    "    batch_min = config.get(\"batch_size_min\", 32)\n",
    "    batch_max =config.get(\"batch_size_max\", 256)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\",\n",
    "                                   [2**i for i in range(int(np.log2(batch_min)), int(np.log2(batch_max))+1)]\n",
    "                                   )\n",
    "\n",
    "    epochs = trial.suggest_int(\"epochs\", config.get(\"epochs_min\", 10), config.get(\"epochs_max\",30))\n",
    "    # patience = trial.suggest_int(\"patience\", config.get(\"patience_min\", 5), config.get(\"patience_max\", 15))\n",
    "    patience = 5\n",
    "\n",
    "    # Define optimizer and loss function\n",
    "    criterion = config.get(\"criterion\", nn.CrossEntropyLoss()) # Classification.\n",
    "\n",
    "    project = config.get(\"project\", 'deep_van_gogh_default')\n",
    "    # Train the model and get the best mean val_auc\n",
    "    mean_dict = cross_validation(learning_rate, weight_decay, num_layers_finetune, criterion, epochs, patience, device, architecture, batch_size, trial, project=project)\n",
    "    # Log the mean values\n",
    "\n",
    "    wandb.init(project = project,\n",
    "                       config={ \"learning_rate\": learning_rate,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"num_layers_finetune\": num_layers_finetune,\n",
    "                                \"trial\": trial.number + 1,\n",
    "                                \"architecture\": architecture,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{architecture}_trial_{trial.number + 1 if trial else -1}\")\n",
    "\n",
    "    wandb.log(mean_dict)\n",
    "    wandb.finish()\n",
    "    # Return best validation loss as the objective to minimize\n",
    "    return mean_dict['Validation AUC']\n"
   ],
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Cross-Validation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "n_trials = 15\n",
    "study = optuna.create_study(study_name=VGG19, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, VGG19, config={'project': 'deep_van_gogh_cross'}), n_trials=n_trials)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:11:15.965256Z",
     "start_time": "2025-03-31T06:11:15.938930Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print('Best trial:')\n",
    "hyperparameters = torch.load(f\"models/AlexNet/AlexNet_best_model_trial_2_fold_1.pt\",\n",
    "                            weights_only=False)['hyperparameters']\n",
    "hyperparameters"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-30T07:24:21.854530Z",
     "start_time": "2025-03-30T07:24:18.294058Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the model - VGG19\n",
    "base_model = vgg19\n",
    "vgg_model = FinedTunedModel(base_model.to(device), ALEXNET).to(device)\n",
    "weight_decay = hyperparameters['weight_decay']\n",
    "lr = hyperparameters['learning_rate']\n",
    "batch_size = hyperparameters['batch_size']\n",
    "epochs = hyperparameters['epochs']\n",
    "patience = hyperparameters['patience']\n",
    "optimizer = optim.Adam(vgg_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "wandb.init(project='Final_Models_Run',\n",
    "                       config={ \"learning_rate\": lr,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": VGG19,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{VGG19}-best\")\n",
    "\n",
    "train_model_with_hyperparams(vgg_model, train_loader, val_loader, optimizer, criterion, epochs=epochs, patience=patience,device=device, trial=None, architecture=ALEXNET, fold=-1, save_model=True, log=True)\n",
    "\n",
    "best_values = train.validation(vgg_model, criterion, test_loader, device, is_test=True)\n",
    "wandb.log(best_values)\n",
    "wandb.finish()\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIb8Ud6FiGuu"
   },
   "source": [
    "# Fine tuning AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "n_trials = 15\n",
    "study = optuna.create_study(study_name=ALEXNET, direction='maximize')\n",
    "study.optimize(lambda trial: objective(trial, ALEXNET, config={'project':'AlexNet-31'}), n_trials=n_trials)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training the model - ALEXNET"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:08:34.454346Z",
     "start_time": "2025-03-31T06:08:34.225698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('Best trial:')\n",
    "hyperparameters = torch.load(f\"models/AlexNet/AlexNet_best_model_trial_2_fold_1.pt\",\n",
    "                            weights_only=False)['hyperparameters']\n",
    "hyperparameters"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'architecture': 'AlexNet',\n",
       " 'epochs': 12,\n",
       " 'patience': 5,\n",
       " 'optimizer': 'Adam',\n",
       " 'learning_rate': 1.1133050164252616e-05,\n",
       " 'weight_decay': 3.732483969944937e-06,\n",
       " 'batch_size': 64}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T06:12:44.276373Z",
     "start_time": "2025-03-31T06:11:30.706368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training the model - ALEXNET\n",
    "base_model = alexnet\n",
    "alexnet_model = FinedTunedModel(base_model.to(device), ALEXNET).to(device)\n",
    "weight_decay = hyperparameters['weight_decay']\n",
    "lr = hyperparameters['learning_rate']\n",
    "batch_size = hyperparameters['batch_size']\n",
    "epochs = hyperparameters['epochs']\n",
    "patience = hyperparameters['patience']\n",
    "optimizer = optim.Adam(alexnet_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_loader = DataLoader(augmented_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "wandb.init(project='Final_Models_Run',\n",
    "                       config={ \"learning_rate\": lr,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": ALEXNET,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{ALEXNET}-best\")\n",
    "\n",
    "train_model_with_hyperparams(alexnet_model, train_loader, val_loader, optimizer, criterion, epochs=epochs, patience=patience,device=device, trial=None, architecture=ALEXNET, fold=-1, save_model=True, log=True)\n",
    "\n",
    "best_values = train.validation(alexnet_model, criterion, test_loader, device, is_test=True)\n",
    "wandb.log(best_values)\n",
    "wandb.finish()\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>▁▂▂▃▄▄▅▅▆▇▇█</td></tr><tr><td>Test AUC</td><td>▁</td></tr><tr><td>Test Accuracy</td><td>▁</td></tr><tr><td>Test F1</td><td>▁</td></tr><tr><td>Test Loss</td><td>▁</td></tr><tr><td>Test Precision</td><td>▁</td></tr><tr><td>Test Recall</td><td>▁</td></tr><tr><td>Test Specificity</td><td>▁</td></tr><tr><td>Train Accuracy</td><td>▁▃▄▅▆▆▇▇▇███</td></tr><tr><td>Train Loss</td><td>█▅▄▃▃▂▂▂▂▁▁▁</td></tr><tr><td>Validation AUC</td><td>▁▅▇▇▇███████</td></tr><tr><td>Validation Accuracy</td><td>▁▂▅▆▆▆▇█▇██▇</td></tr><tr><td>Validation F1</td><td>▁▃▅▆▇▆▇█▇██▇</td></tr><tr><td>Validation Loss</td><td>█▅▃▂▂▁▁▁▁▁▁▂</td></tr><tr><td>Validation Precision</td><td>▁▂▅▆▆▆▇█▇██▇</td></tr><tr><td>Validation Recall</td><td>▁▂▅▆▆▆▇█▇██▇</td></tr><tr><td>Validation Specificity</td><td>█▇▄▇▁▄▂▄▄▄▂▂</td></tr><tr><td>batch_size</td><td>▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Epoch</td><td>12</td></tr><tr><td>Test AUC</td><td>0.92811</td></tr><tr><td>Test Accuracy</td><td>0.90376</td></tr><tr><td>Test F1</td><td>0.89359</td></tr><tr><td>Test Loss</td><td>0.44048</td></tr><tr><td>Test Precision</td><td>0.91274</td></tr><tr><td>Test Recall</td><td>0.90376</td></tr><tr><td>Test Specificity</td><td>0.99803</td></tr><tr><td>Train Accuracy</td><td>0.99436</td></tr><tr><td>Train Loss</td><td>0.02037</td></tr><tr><td>Validation AUC</td><td>0.95975</td></tr><tr><td>Validation Accuracy</td><td>0.96443</td></tr><tr><td>Validation F1</td><td>0.96227</td></tr><tr><td>Validation Loss</td><td>0.11247</td></tr><tr><td>Validation Precision</td><td>0.96201</td></tr><tr><td>Validation Recall</td><td>0.96443</td></tr><tr><td>Validation Specificity</td><td>0.98969</td></tr><tr><td>batch_size</td><td>64</td></tr></table><br/></div></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">AlexNet-best</strong> at: <a href='https://wandb.ai/amitr5-tel-aviv-university/Final_Models_Run/runs/hengvaeh' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/Final_Models_Run/runs/hengvaeh</a><br> View project at: <a href='https://wandb.ai/amitr5-tel-aviv-university/Final_Models_Run' target=\"_blank\">https://wandb.ai/amitr5-tel-aviv-university/Final_Models_Run</a><br>Synced 5 W&B file(s), 0 media file(s), 2 artifact file(s) and 1 other file(s)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250331_090839-hengvaeh\\logs</code>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 83
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "analysing results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-31T05:09:26.726341Z",
     "start_time": "2025-03-31T05:09:26.712339Z"
    }
   },
   "source": [
    "wandb.init(project='Final_Models',\n",
    "                       config={ \"learning_rate\": lr,\n",
    "                                \"weight_decay\": weight_decay,\n",
    "                                \"patience\": patience,\n",
    "                                \"batch_size\": batch_size,\n",
    "                                \"epochs\": epochs,\n",
    "                                \"architecture\": ALEXNET,\n",
    "                                \"dataset\": \"Post_Impressionism\",\n",
    "                                }, name=f\"{ALEXNET}-best\")"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMIPaYdChtVL"
   },
   "source": [
    "# Style transfer function"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "gy8lmbPmhj9H",
    "outputId": "330638de-9f78-4d50-c423-bbab448afc37",
    "ExecuteTime": {
     "end_time": "2025-03-31T05:59:50.441843Z",
     "start_time": "2025-03-31T05:59:50.401842Z"
    }
   },
   "source": "best_values['Confusion_matrix']",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CustomChart' object has no attribute 'plot'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[55], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mbest_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mConfusion_matrix\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mplot\u001B[49m()\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'CustomChart' object has no attribute 'plot'"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "2cMigqU0CGg5",
    "outputId": "afdc3a89-f2ad-457b-eeb7-b4aabcf214ca",
    "ExecuteTime": {
     "end_time": "2025-03-31T06:03:49.434487Z",
     "start_time": "2025-03-31T06:03:49.376004Z"
    }
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def plot_wandb_confusion_matrix(wandb_cm_object, class_names=[\"Not Van Gogh\", \"Van Gogh\"]):\n",
    "    \"\"\"\n",
    "    Takes a wandb confusion matrix object and plots it locally in PyCharm\n",
    "    \"\"\"\n",
    "    # Extract the confusion matrix data from wandb object\n",
    "    # The data is stored in the 'table' property\n",
    "    cm_data = np.array(wandb_cm_object.table.data)\n",
    "    print(cm_data)\n",
    "    # Reshape to a 2x2 matrix, skipping the first row and column (labels)\n",
    "    cm = np.zeros((2, 2))\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            cm[i, j] = cm_data[i+1][j+1]\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with your wandb confusion matrix\n",
    "# Assuming you have the wandb confusion matrix in val_metrics['Confusion_matrix']\n",
    "plot_wandb_confusion_matrix(best_values['Confusion_matrix'])"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Not Van Gogh' 'Not Van Gogh' '3539.0']\n",
      " ['Not Van Gogh' 'Van Gogh' '8.0']\n",
      " ['Van Gogh' 'Not Van Gogh' '453.0']\n",
      " ['Van Gogh' 'Van Gogh' '551.0']]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Van Gogh'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[59], line 30\u001B[0m\n\u001B[0;32m     26\u001B[0m     plt\u001B[38;5;241m.\u001B[39mshow()\n\u001B[0;32m     28\u001B[0m \u001B[38;5;66;03m# Example usage with your wandb confusion matrix\u001B[39;00m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# Assuming you have the wandb confusion matrix in val_metrics['Confusion_matrix']\u001B[39;00m\n\u001B[1;32m---> 30\u001B[0m \u001B[43mplot_wandb_confusion_matrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbest_values\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mConfusion_matrix\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[59], line 17\u001B[0m, in \u001B[0;36mplot_wandb_confusion_matrix\u001B[1;34m(wandb_cm_object, class_names)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m---> 17\u001B[0m         cm[i, j] \u001B[38;5;241m=\u001B[39m cm_data[i\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m][j\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     19\u001B[0m \u001B[38;5;66;03m# Create the plot\u001B[39;00m\n\u001B[0;32m     20\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m8\u001B[39m, \u001B[38;5;241m6\u001B[39m))\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'Van Gogh'"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2UNqmGA2DXDj",
    "outputId": "18e5c745-d27d-4f23-97cf-717b194bd3c3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-dC9MS7V6WZF",
    "outputId": "80757501-0728-4fe1-96b7-a15122faad92"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w93PNhRgHVue",
    "outputId": "704e1556-8d2f-4c90-af0e-bc26aa7a072e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24UKrxg-H25c"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
